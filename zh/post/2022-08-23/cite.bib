@inproceedings{hu-etal-2021-mmgcn,
    title = "{MMGCN}: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
    author = "Hu, Jingwen  and
      Liu, Yuchen  and
      Zhao, Jinming  and
      Jin, Qin",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.440",
    doi = "10.18653/v1/2021.acl-long.440",
    pages = "5666--5675",
    abstract = "Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users{'} emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.",
}

@misc{https://doi.org/10.48550/arxiv.2206.02187,
  doi = {10.48550/ARXIV.2206.02187},
  url = {https://arxiv.org/abs/2206.02187},
  author = {Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation},
  publisher = {arXiv},
  year = {2022}, 
  copyright = {arXiv.org perpetual, non-exclusive license}
}