[{"authors":null,"categories":null,"content":"Life is like a train, a train bound for the tomb, there will be a lot of stations on the road, it was difficult to keep accompany to walk. When you get out, even if is not, the grateful, then waved goodbye. When goodbye, never say goodbye.\nLove life, love to study, dare to try, dare to challenge, never give up!\n Download my resumÃ©. -- ","date":1690934400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690934400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Life is like a train, a train bound for the tomb, there will be a lot of stations on the road, it was difficult to keep accompany to walk. When you get out, even if is not, the grateful, then waved goodbye.","tags":null,"title":"Chuanbo Zhu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://2oil.top/en/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Chuanbo Zhu","Min Chen","Sheng Zhang","Chao Sun","Han Liang","Yifan Liu","Jincai Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1690934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690934400,"objectID":"1507621ea8758022de1c4e2bd0df7d30","permalink":"https://2oil.top/en/publication/inffus2023/","publishdate":"2023-08-02T00:00:00Z","relpermalink":"/en/publication/inffus2023/","section":"publication","summary":"Propose a sentiment knowledge-enhanced attention fusion network (SKEAFN); Build an additional knowledge graph to leverage explicit sentiment information; Use a multi-head attention mechanism to model the interactions among modalities; Develop feature-wised attention to adjust the contributions of multiple modalities.","tags":[],"title":"SKEAFN: Sentiment Knowledge Enhanced Attention Fusion Network for multimodal sentiment analysis","type":"publication"},{"authors":["Yifan Liu","Jincai Chen","Ping Lu","Chuanbo Zhu","Yugen Jian","Chao Sun","Han Liang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1685664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685664000,"objectID":"d1e8150e98df6424bcf60630a5c2c6ea","permalink":"https://2oil.top/en/publication/caj2023/","publishdate":"2023-06-02T00:00:00Z","relpermalink":"/en/publication/caj2023/","section":"publication","summary":"An efficient desnowing method named Snowed Autoencoder (SAE) is proposed; A multi-encoder for SAE using snow information with an attention module is proposed; A multi-decoder for SAE with fewer sub-decoders than sub-encoders is proposed; SAE outperformed state-of-the-art on four desnowing datasets.","tags":[],"title":"Snowed autoencoders are efficient snow removers","type":"publication"},{"authors":["Yifan Liu","Jincai Chen","Ping Lu","Chuanbo Zhu","Yugen Jian","Chao Sun","Han Liang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1681948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681948800,"objectID":"78607c35d40ef2128320e44b412a9588","permalink":"https://2oil.top/en/publication/tjsc2023/","publishdate":"2023-04-20T00:00:00Z","relpermalink":"/en/publication/tjsc2023/","section":"publication","summary":"Deep learning-based methods have achieved excellent performance in image-deraining tasks. Unfortunately, most existing deraining methods incorrectly assume a uniform rain streak distribution and a fixed fine-grained level. And this uncertainty of rain streaks will result in the model not being competent at repairing all fine-grained rain streaks. In addition, some existing convolution-based methods extend the receptive field mainly by stacking convolution kernels, which frequently results in inaccurate feature extraction. In this work, we propose momentum-contrast and large-kernel for multi-fine-grained deraining network (MOONLIT). To address the problem that the model is not competent at all fine-grained levels, we use the unsupervised dictionary contrastive learning method to treat different fine-grained rainy images as different degradation tasks. Then, to address the problem of inaccurate feature extraction, we carefully constructed a restoration network based on large-kernel convolution with a larger and more accurate receptive field. In addition, we designed a data enhancement method to weaken features other than rain streaks in order to be better classified for different degradation tasks. Extensive experiments on synthetic and real-world deraining datasets show that the proposed method MOONLIT achieves the state-of-the-art performance on some datasets. Code is available at https://github.com/awhitewhale/moonlit.","tags":[],"title":"MOONLIT: momentum-contrast and large-kernel for multi-fine-grained deraining","type":"publication"},{"authors":["Yifan Liu","Jincai Chen","Ping Lu","Chuanbo Zhu","Yugen Jian","Chao Sun","Han Liang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1677974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677974400,"objectID":"c8d164937f870d35ff4983b8c2bd7b19","permalink":"https://2oil.top/en/publication/displays2023/","publishdate":"2023-03-05T00:00:00Z","relpermalink":"/en/publication/displays2023/","section":"publication","summary":"Multi-scaled feature-fused image dehazing network called MFID-Net is proposed; MFID-Net uses dynamic dehazing convolution and activation to improve image dehazing; The proposed dynamic dehazing convolution utilizes dynamic weight fusion; The proposed dynamic dehazing activation uses input global context encoding function; MFID-Net achieves the state-of-the-art performance on dehazing datasets.","tags":[],"title":"MFID-Net: Multi-scaled feature-fused image dehazing via dynamic weights","type":"publication"},{"authors":["Chuanbo Zhu"],"categories":["å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ","å¯¹è¯æƒ…æ„Ÿè¯†åˆ«","æ·±åº¦å›¾å·ç§¯ç½‘ç»œ"],"content":"ä½œè€…å•ä½: ä¸­å›½äººæ°‘å¤§å­¦ é‡‘ç´å›¢é˜Ÿ  [è®ºæ–‡é“¾æ¥](https://aclanthology.org/2021.acl-long.440.pdf)  Abstract: Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand usersâ€™ emotions and generate empathetic re- sponses. However, most works focus on mod- eling speaker and contextual information pri- marily on the textual modality or simply lever- aging multimodal information through fea- ture concatenation. In order to explore a more effective way of utilizing both multi- modal and long-distance contextual informa- tion, we propose a new model based on mul- timodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effec- tively, but also leverage speaker information to model inter-speaker and intra-speaker de- pendency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effec- tiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting. -- 1. ERCä»»åŠ¡å®šä¹‰ã€åº”ç”¨åœºæ™¯ã€å…³é”®æŒ‘æˆ˜ ERCä»»åŠ¡æ˜¯æƒ…æ„Ÿå¯¹è¯ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å¹¶è·Ÿè¸ªå¯¹è¯æœŸé—´è¯´è¯è€…çš„æƒ…ç»ªçŠ¶æ€ï¼Œå¯å¹¿æ³›ç”¨äºç”µå­åŒ»ç–—æœåŠ¡ã€æ™ºèƒ½å®¢æœç­‰ä¼—å¤šäººæœºäº¤äº’é¢†åŸŸã€‚å…¶å…³é”®æŠ€æœ¯æŒ‘æˆ˜åŒ…æ‹¬ï¼š1ï¼‰å¦‚ä½•å¯¹conversationçš„åŸºæœ¬ç»„æˆå•å…ƒutteranceè¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ï¼›2ï¼‰å¦‚ä½•åˆ©ç”¨speakerä¿¡æ¯ï¼›3ï¼‰å¦‚ä½•è·å–å¹¶åˆ©ç”¨conversationçš„topicä¿¡æ¯ï¼ˆé’ˆå¯¹conversationä¸­topicå˜åŒ–çš„æƒ…å½¢ï¼‰ï¼›4ï¼‰å¦‚ä½•åˆ©ç”¨å…¶å®ƒæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¦‚è§†è§‰/å¬è§‰æ¨¡æ€ç­‰ã€‚\n2. å¸¸ç”¨æ•°æ®é›† MELD/IEMOCAPæ˜¯ä¸¤ä¸ªå¸¸ç”¨çš„å¤šæ¨¡æ€ERCæ•°æ®é›†ï¼Œå…¶ç»Ÿè®¡ä¿¡æ¯å¦‚ä¸‹æ‰€ç¤ºï¼š\n   æ•°æ®é›† conversation (Train/Valid/Test) utterance (Train/Valid/Test)     MELD 1038/114/280 9989/1109/2610 (13708)   IEMOCAP 100/20/31 4810/1000/1623 (7433)    æ³¨æ„ï¼šä¸åŒè®ºæ–‡åˆ—å‡ºçš„æ ·æœ¬æ•°å¹¶ä¸ç›¸åŒï¼ˆå¦‚ACL2021çš„MMGCNã€CVPR2022çš„M2FNetã€ICASSP2022çš„CKE-Netï¼Œéœ€è¦æ ¹æ®æ•°æ®é›†å®é™…çš„å¤„ç†æƒ…å†µè¿›è¡Œæ›´æ”¹ã€‚\nconversationä¸­utteranceä¸ªæ•°ç»Ÿè®¡\n   æ•°æ®é›† Max Min Mean     MELD 24 1 9.62   IEMOCAP       conversationä¸­æ–‡æœ¬æ¨¡æ€çš„é•¿åº¦ç»Ÿè®¡\n   æ•°æ®é›† Max Min Mean     MELD 321 1 77.33   IEMOCAP       MELDæ•°æ®é›†ï¼šåŒ…å«æ–‡æœ¬ã€è§†è§‰ã€å¬è§‰ä¸‰ç§æ¨¡æ€çš„é«˜è´¨é‡å¯¹é½æ•°æ®ï¼ŒåŒ…å«304ä¸ªä¸åŒçš„speakersï¼Œæ¯ä¸ªconversationéƒ½åŒ…å«ä¸‰ä¸ªåŠä¸‰ä¸ªä»¥ä¸Šçš„speakersã€‚æ‰€æœ‰çš„utteranceéƒ½æ ‡æ³¨äº†ä¸ƒç±»emotion (anger, disgust, fear, joy, neutral, sadness, and surprise)å’Œä¸‰ç±»sentiment (neutral, positive, and negative)æ ‡ç­¾ã€‚åŸå§‹æ•°æ®é›†å·²åŒ…å«Validå’ŒTestã€‚\næ•ˆæœæ’è¡Œæ¦œï¼šhttps://paperswithcode.com/sota/emotion-recognition-in-conversation-on-meld\nIEMOCAPæ•°æ®é›†ï¼šåŒ…å«æ–‡æœ¬ã€è§†è§‰ã€å¬è§‰ä¸‰ç§æ¨¡æ€çš„è¿‘12å°æ—¶two-way conversationsæ•°æ®ï¼ŒåŒ…å«10ä¸ªspeakersã€‚æ‰€æœ‰çš„utteranceéƒ½æ ‡æ³¨äº†å…­ç±»emotion (happy, sad, neutral, angry, excited, and frustrated)æ ‡ç­¾ã€‚åŸå§‹æ•°æ®é›†æ— Validï¼ŒCVPR2022çš„M2FNetæ˜¯ä»Trainé›†åˆä¸­éšæœºé€‰å–10%ä½œä¸ºValidã€‚\nQ1: åŸå§‹æ•°æ®é›†æ²¡æœ‰ç»™å‡ºconversationæ ‡ç­¾ï¼Œè€ŒERCä»»åŠ¡åˆæ˜¯é’ˆå¯¹å¯¹è¯çš„æƒ…ç»ªè¯†åˆ«ï¼Œåº”å¦‚ä½•ç†è§£ï¼Ÿå…ˆå‰è®ºæ–‡å¦‚ä½•å¤„ç†ï¼Ÿ A1: ERCä»»åŠ¡çš„æ€§èƒ½è¯„ä»·åº”è½è„šåˆ°utterancesã€‚ä¸ä¸€èˆ¬ERä¸åŒçš„æ˜¯ï¼ŒERCéœ€è¦æ›´å¤šåœ°è€ƒè™‘utteranceçš„ä¸Šä¸‹æ–‡åœºæ™¯ã€‚æ¯”å¦‚MMGCNè®­ç»ƒæ—¶çš„è¾“å…¥å°±æ˜¯batch (16) ä¸ªconversationsã€‚ \n3. è¯„ä»·æŒ‡æ ‡  CVPR2022çš„M2FNet: accuracy / weighted average F1 score ICASSP2022çš„CKE-Net: weighted F1 score ACL2021çš„MMGCN: weighted accuracy / weighted average F1 score  4. ç‰¹å¾æå– æ³¨æ„ï¼šåŸå§‹æ•°æ®é›†æ²¡æœ‰æä¾›ç»Ÿä¸€çš„ç‰¹å¾ï¼Œä¸åŒè®ºæ–‡çš„å¤„ç†æ–¹å¼ä¸ä¸€è‡´ã€‚è¿™é‡Œå°±å­˜åœ¨é—®é¢˜ï¼Œè®ºæ–‡æŒ‡æ ‡é«˜å¹¶ä¸ä¸€å®šæ˜¯æ¨¡å‹å¥½ï¼Œä¹Ÿå¯èƒ½æ˜¯ç‰¹å¾å¥½ã€‚\n CVPR2022çš„M2FNet: æ–‡æœ¬æ¨¡æ€ä½¿ç”¨RoBERTaï¼Œè§†è§‰/å¬è§‰æ¨¡æ€ä½¿ç”¨ResNet18ï¼Œvisual extractoråœ¨CASIA webface databaseä¸Šè®­ç»ƒçš„ï¼Œaudio extractoræ˜¯ä»ç›¸åº”éŸ³é¢‘ä¿¡å·è·å¾—çš„æ¢…å°”é¢‘è°±ä¸Šè®­ç»ƒçš„ã€‚ä½œè€…è¿˜æ„å»ºäº†ä¸€ä¸ªç”¨äºç‰¹å¾æå–æ¨¡å‹è®­ç»ƒçš„æŸå¤±å‡½æ•°ï¼ˆç»¼åˆè¾¹é™…ä¸‰å…ƒç»„$L_{AMT}$ã€åæ–¹å·®$L_{Cov}$å’Œæ–¹å·®$L_{Var}$çš„æŸå¤±ï¼‰ã€‚è¡¨ç¤ºä¸ºï¼š$L_{FE}={\\lambda_1}{L_{AMT}} + {\\lambda_2}{L_{Cov}} + {\\lambda_3}{L_{Var}}$ ICASSP2022çš„CKE-Net: æ— è¯¦ç»†ä»‹ç»ã€‚ ACL2021çš„MMGCN: æ–‡æœ¬æ¨¡æ€é‡‡ç”¨TextCNNï¼Œè§†è§‰æ¨¡æ€ä½¿ç”¨åœ¨FER+æ•°æ®é›†ä¸Šå¾®è°ƒçš„DenseNetï¼Œå¬è§‰æ¨¡æ€é‡‡ç”¨OpenSmile toolkit with IS10ã€‚  5. ç›¸å…³å·¥ä½œ 5.1 MMGCN (ACL2021, Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation)  ä½œè€…å•ä½: ä¸­å›½äººæ°‘å¤§å­¦ é‡‘ç´å›¢é˜Ÿ è®ºæ–‡é“¾æ¥\nAbstract: Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand usersâ€™ emotions and generate empathetic re- sponses. However, most works focus on mod- eling speaker and contextual information pri- marily on the textual modality or simply lever- aging multimodal information through fea- ture concatenation. In order to explore a more effective way of utilizing both multi- modal and long-distance contextual informa- tion, we propose a new model based on mul- timodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effec- tively, but also leverage speaker information to model inter-speaker and intra-speaker de- pendency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effec- tiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.\n åˆ›æ–°ç‚¹ï¼šæå‡ºä¸€ç§å¤šæ¨¡æ€èåˆå›¾å·ç§¯ç½‘ç»œï¼Œåœ¨å¤šæ¨¡æ€èåˆçš„åŒæ—¶æ•è·é•¿è·ç¦»ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\nå›¾å·ç§¯å…¬å¼ï¼š\n$\\widetilde{\\mathcal{P}}=\\widetilde{\\mathcal{D}}^{-1/2}\\widetilde{\\mathcal{A}}\\widetilde{\\mathcal{D}}^{-1/2}={(\\mathcal{D}+\\mathcal{L})}^{-1/2}{(\\mathcal{A}+\\mathcal{L})}{(\\mathcal{D}+\\mathcal{L})}^{-1/2}$\n$\\mathcal{H}^{(l+1)}=\\sigma({({(1-\\alpha)\\widetilde{\\mathcal{P}}\\mathcal{H}^{(l)} + \\alpha\\mathcal{H}^{(0)}})}{({(1-\\beta^{l})\\mathcal{L}+\\beta^{(l)}\\mathcal{W}^{(l)}})})$\n$\\beta^{(l)}=\\log{(\\frac{\\eta}{l} + 1)}$\n$\\mathcal{L}$æ˜¯æ’ç­‰æ˜ å°„ (identify mapping)ï¼Œæ˜¯ä¸€ç§æ®‹å·®æœºåˆ¶\n","date":1661212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661293200,"objectID":"63cac6cf7db6a60e9a95aa24b40bf5c1","permalink":"https://2oil.top/en/post/2022-08-23/","publishdate":"2022-08-23T00:00:00Z","relpermalink":"/en/post/2022-08-23/","section":"post","summary":"å¯¹è¯ä¸­çš„æƒ…ç»ªè¯†åˆ«","tags":["å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ","å¯¹è¯æƒ…æ„Ÿè¯†åˆ«","æ·±åº¦å›¾å·ç§¯ç½‘ç»œ"],"title":"è®ºæ–‡é˜…è¯»â€”â€”ERCä»»åŠ¡ç»¼è¿°","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ","æ¨¡æ€å¯¹é½","è·¨æ¨¡æ€æ£€ç´¢","è§†é¢‘æè¿°ç”Ÿæˆ"],"content":"1. Motivation ä½œè€…è®¤ä¸ºï¼šå¾ˆå¤šè§†é¢‘-æ–‡æœ¬ä»»åŠ¡éƒ½è•´å«ç€ä¸åŒç²’åº¦çš„ä¿¡æ¯ï¼Œå¦‚ï¼šè§†é¢‘å¸§-å•è¯ã€ç‰‡æ®µ-å¥å­ã€è§†é¢‘-æ®µè½ï¼Œæ¯ä¸ªéƒ½åŒ…å«äº†ä¸åŒç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚\næœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯ï¼š1ï¼‰å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å±‚æ¬¡åŒ–çš„ä¿¡æ¯ï¼›2ï¼‰å¦‚ä½•å»ºæ¨¡ä¸åŒç²’åº¦ã€ä¸åŒæ¨¡æ€æ•°æ®ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚\næå‡ºçš„COOTæ¨¡å‹åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š\n ä¸€ä¸ªæ³¨æ„åŠ›æ„ŸçŸ¥èšåˆå±‚ï¼Œç”¨äºlocal temporal context (intra-level, within a clip)ï¼› ä¸€ä¸ªä¸Šä¸‹æ–‡transformerï¼Œç”¨äºå­¦ä¹ ä½çº§è¯­ä¹‰å’Œé«˜çº§è¯­ä¹‰çš„äº¤äº’(inter-level, clip-video/sentence-paragraph)ï¼› ç”¨äºè·¨æ¨¡æ€äº¤äº’çš„å¾ªç¯ä¸€è‡´æ€§æŸå¤± (cycle-consistency loss)   Abstract: Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext\n 2. Model 3. Cross-Modal Cycle Consistency Loss (è·¨æ¨¡æ€å¾ªç¯ä¸€è‡´æ€§) å¾ªç¯æ„å‘³ç€é—­ç¯ï¼Œè¯¥æŸå¤±ç”¨äºå­¦ä¹ è·¨æ¨¡æ€æ•°æ®çš„è¯­ä¹‰å¯¹é½ã€‚å…¶åŸºæœ¬å‡è®¾ä¸ºï¼šâ€œA pair of clip and sentence will be identiï¬ed as semantically aligned if they are nearest neighbors in the learned common spaces.â€ã€‚æ¢è¨€ä¹‹ï¼šå°†clipå’Œsentenceå‘é‡æ˜ å°„åˆ°common sapceä¸­ï¼Œåˆ™è¿™ä¸¤ä¸ªå‘é‡æ˜¯æœ€ç›¸é‚»çš„ã€‚\nå®šä¹‰clipåºåˆ—ï¼š$[{\\theta_{i}}]_{i=1}^{n} = [\\theta_{1}, \\cdots, \\theta_{n}]$\nå®šä¹‰sentenceåºåˆ—ï¼š$[{\\delta_{i}}]_{i=1}^{m} = [\\delta{1}, \\cdots, \\delta{m}]$\nç»™å®šsentence ${\\delta_{i}}$ï¼Œå¯ä»¥è®¡ç®—å‡ºæœ€è¿‘é‚» (soft nearest neighbor): $$ \\bar{\\theta_{\\delta_{i}}}=\\sum_{j=1}^{n}\\alpha_{j}\\theta_{j},\\\n$$ å…¶ä¸­ï¼Œ$\\alpha_{j}$æ˜¯clip $\\theta_{j}$å¯¹sentence $\\delta_{i}$çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼š $$ \\alpha_{j} = \\frac{\\exp(-{||\\delta_{i}-\\theta_{j}||^2})}{\\sum_{k=1}^n{\\exp(-{||\\delta_{i}-\\theta_{k}||}^2)}} $$\nç»™å®š$\\bar{\\theta_{\\delta_{i}}}$ï¼Œåè¿‡æ¥å¯ä»¥è®¡ç®—soft locationï¼š $$ \\mu = \\sum_{j=1}^{m}\\beta_{j}j $$ å…¶ä¸­ï¼Œ$\\beta_{j}$æ˜¯sentence $\\delta_{k}$å¯¹äºclipå‘é‡$\\bar{\\theta_{\\delta_{i}}}$çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼š $$ \\beta_{j} = \\frac{\\exp(-{||\\bar{\\theta_{\\delta_{i}}}-\\delta_{j}||}^2)}{\\sum_{k=1}^{m}\\exp(-{||\\bar{\\theta_{\\delta_{i}}}-\\delta_{k}||}^2)} $$ æ­£åå‘æ“ä½œåï¼Œå½“ä¸”ä»…å½“$\\mu=i$æ—¶ï¼Œæ‰æ˜¯è·¨æ¨¡æ€å¾ªç¯ä¸€è‡´çš„ã€‚åŸæ–‡ä¸ºï¼šThe sentence embedding $\\delta_{i}$ is semantically cycle consistent if and only if it cycles back to the original location, i.e., $i=\\mu$ã€‚\nç”±æ­¤ï¼Œå¯å¾—è·¨æ¨¡æ€å¾ªç¯ä¸€è‡´æ€§æŸå¤±$l_{CMC}$ $$ l_{CMC} = ||i-\\mu||^2 $$\næ€»ç»“ï¼š$l_{CMC}$ä»å¤šæ¨¡æ€è¯­ä¹‰å¯¹é½çš„åŸºæœ¬æ¦‚å¿µå‡ºå‘ï¼Œé€šè¿‡$A\\rightarrow B, B\\rightarrow A$çš„å¾ªç¯æ˜ å°„æ¥å¢å¼ºä¸¤ä¸ªæ¨¡æ€çš„ä¸€è‡´æ€§ï¼ˆè¿›è¡Œå¤šæ¨¡æ€è¯­ä¹‰å¯¹é½ï¼‰ã€‚å…¶å¤„ç†å¯¹è±¡ä¸ºåºåˆ—é•¿åº¦ä¸ä¸€è‡´çš„sentenceså’Œclipså¯¹ï¼Œæ˜¯éæ˜¾å¼å¯¹é½çš„æ–¹æ³•ï¼Œæ„Ÿè§‰å¯ä»¥ç”¨åœ¨ä¹‹åçš„å·¥ä½œä¸­ã€‚\n4. Result æ³¨æ„ï¼šä¸‹åˆ—å›¾ç‰‡ä¸­$R@N$æŒ‡æ ‡è¡¨ç¤ºtop Nç»“æœçš„å¬å›ç‡ã€‚\n  http://pelhans.com/2019/04/08/deep_learning-note0/#rn https://blog.csdn.net/hei653779919/article/details/105932096   ","date":1660521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660602000,"objectID":"65850441185b41681360cc9d117fd5ac","permalink":"https://2oil.top/en/post/2022-08-15/","publishdate":"2022-08-15T00:00:00Z","relpermalink":"/en/post/2022-08-15/","section":"post","summary":"è®ºæ–‡å¤ç°","tags":["å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ","æ¨¡æ€å¯¹é½","è·¨æ¨¡æ€æ£€ç´¢","è§†é¢‘æè¿°ç”Ÿæˆ"],"title":"å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ï¼ˆè·¨æ¨¡æ€æ£€ç´¢â€”â€”è§†é¢‘æè¿°ç”Ÿæˆï¼‰","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ","è¯­éŸ³æƒ…æ„Ÿåˆ†æ"],"content":"[TOC]\nICASSP 2022è¯­éŸ³æƒ…æ„Ÿåˆ†æè®ºæ–‡åˆ—è¡¨  Representation Learning Through Cross-modal Conditional Teacher-student Training For Speech Emotion Recognition, Amazon AWS AI Speech Emotion Recognition Using Self-supervised Features, IBM Research AI Multi-stage Graph Representation Learning For Dialogue-level Speech Emotion Recognition, å¤©æ´¥å¤§å­¦ Frontend Attributes Disentanglement For Speech Emotion Recognition, ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦, iFLYTEK Neural Architecture Search For Speech Emotion Recognition, é¦™æ¸¯ä¸­æ–‡å¤§å­¦ Climate and Weather: Inspecting Depression Detection Via Emotion Recognition, å‰‘æ¡¥å¤§å­¦ A Commonsense Knowledge Enhanced Network with Retrospective Loss for Emotion Recognition in Spoken Dialog, å“ˆå·¥å¤§  1. Representation Learning Through Cross-modal Conditional Teacher-student Training For Speech Emotion Recognition  ä½œè€…å•ä½: Amazon AWS AI; è®ºæ–‡é“¾æ¥\n â€ƒè™½ç„¶æ–‡æœ¬å’Œè¯­éŸ³çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å‡å°‘å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ï¼Œä½†æ˜¯å¦‚ä½•åœ¨emotion recognitionä»»åŠ¡ä¸­ä½¿ç”¨åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„representationè¿˜å°šåœ¨ç ”ç©¶ã€‚æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼š1ï¼‰top-performing representationsåœ¨emotion recognitionä¸­çš„ä¸»è¦å·®å¼‚åœ¨äºvalence (Vç»´)ï¼Œè€Œactivation (Aç»´)å’Œdominance (Dç»´)çš„å·®å¼‚ä¸å¤§ï¼›2ï¼‰åœ¨valenceé¢„æµ‹ä¸­ï¼Œå³ä½¿æœ€ä¼˜çš„HUBERT representationä¹Ÿå¼±äºtext-speech representationã€‚\nâ€ƒæœ¬æ–‡é€šè¿‡condition teacher-Student trainingçš„æ–¹å¼æ¥å‘speech representationèå…¥lexical informationã€‚å®éªŒç”¨åˆ°çš„æ•°æ®é›†åŒ…æ‹¬MSP-Podcast corpuså’ŒIEMOCAPï¼Œconcordance correlation coefficient (CCC)æŒ‡æ ‡åœ¨audio-onlyåœºæ™¯ä¸‹è¾¾åˆ°äº†SOTAçš„æ•ˆæœã€‚ 2. Speech Emotion Recognition Using Self-supervised Features  ä½œè€…å•ä½: IBM Research AI; è®ºæ–‡é“¾æ¥\n â€ƒæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„ç«¯åˆ°ç«¯(E2E) SERç³»ç»Ÿã€‚è¯¥ç³»ç»ŸåŸºäºä¸€ç§ä¸Šä¸‹æ¸¸æ¶æ„èŒƒå¼ï¼Œå¯ä»¥è½»æ¾ä½¿ç”¨/é›†æˆå„ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å—ã€‚å®é™…ä¸Šï¼Œæœ¬æ–‡æ¨¡å‹å°±æ˜¯äº”æŠ˜äº¤å‰éªŒè¯ + é¢„è®­ç»ƒå¾®è°ƒ + ç‰¹å¾èšåˆï¼ˆMean Average Pooling å’ŒECAPA-TDNNï¼‰ + æ¨¡å‹èšåˆï¼Œæ„Ÿè§‰æœ‰ç‚¹å„¿åƒæ¯”èµ›çš„trickã€‚\nâ€ƒæ•°æ®é›†é‡‡ç”¨çš„æ˜¯IEMOCAPã€‚ 3. Multi-stage Graph Representation Learning For Dialogue-level Speech Emotion Recognition  ä½œè€…å•ä½: å¤©æ´¥å¤§å­¦; è®ºæ–‡é“¾æ¥\n â€ƒéšç€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æŠ€æœ¯çš„å‘å±•ï¼Œç›®å‰çš„ç ”ç©¶å¤§å¤šåœç•™åœ¨è¯è¯­å±‚é¢ï¼Œæ— æ³•é€‚åº”å®é™…åœºæ™¯çš„éœ€è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç€é‡äºæ•è·å¯¹è¯çº§ä¸Šä¸‹æ–‡ä¿¡æ¯çš„emotion recognitionç­–ç•¥ã€‚è¯¥ç­–ç•¥åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼š1ï¼‰å¯¹è¯å¤šé˜¶æ®µå›¾è¡¨ç¤ºå­¦ä¹ ç®—æ³•(DialogMSG)å¼•å…¥äº†ä»ä¸åŒå¯¹è¯èŒƒå›´è¿›è¡Œå»ºæ¨¡çš„å¤šé˜¶æ®µå›¾ï¼Œä»¥è·å–æ›´æœ‰æ•ˆçš„ä¿¡æ¯ï¼›2ï¼‰åŒ…å«utterance-level classifierå’Œ dialogue-level classifierçš„åŒçº¦æŸæ¨¡å—ã€‚\nâ€ƒå®éªŒç”¨åˆ°çš„æ•°æ®é›†æ˜¯IEMOCAPï¼Œdialogue atmosphereæ ‡ç­¾æ˜¯è¯¥è½®dialogçš„æ‰€æœ‰utteranceä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„emotionã€‚ è¿™æ ·åšçš„åŸå› æ˜¯ä½œè€…è®¤ä¸ºå¯¹è¯çš„æ°›å›´ä¸è¯´è¯äººæƒ…ç»ªæœ‰å¾ˆå¼ºçš„å…³ç³»ï¼Œä½†å–æ ‡ç­¾çš„æ–¹å¼æ˜¾å¾—è¿‡äºç²—ç³™ã€‚å°¤å…¶IEMOCAPæ˜¯ä¸ªè¡¨æ¼”å‹æ•°æ®é›†ï¼ˆè¯´è¯äººé’ˆå¯¹ç‰¹å®šçš„æƒ…ç»ªè¿›è¡Œè¡¨æ¼”ï¼Œä¸çœŸå®åœºæ™¯æœ‰å¾ˆå¤§çš„å·®è·ï¼‰ï¼Œdialogue atmosphereæ ‡ç­¾å‡†ç¡®ç‡å¯èƒ½ä¼šå¾ˆä½ã€‚\n4. Frontend Attributes Disentanglement For Speech Emotion Recognition  ä½œè€…å•ä½: ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦, iFLYTEK Research; è®ºæ–‡é“¾æ¥\n â€ƒåŸºäºæœ‰é™æ•°æ®é›†çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè¯­éŸ³ä¿¡æ¯é™¤äº†åŒ…å«æƒ…æ„Ÿå¤–ï¼Œè¿˜åŒ…å«è¯´è¯äººã€è¯­ä¹‰å’Œè¯­ç§ç­‰å„ç§å¹²æ‰°å±æ€§ã€‚ç„¶è€Œï¼Œç”±äºè¯´è¯äººä¸æƒ…æ„Ÿå±æ€§ä¹‹é—´çš„å¯†åˆ‡å…³ç³»ï¼Œåªéœ€å¯¹çº¿æ€§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå°±è¶³ä»¥åœ¨é¢„å…ˆè®­ç»ƒçš„è¯´è¯äººè¯†åˆ«(SR)å‰ç«¯æå–çš„è¯è¯­çº§åµŒå…¥(å³å‘é‡å’Œxå‘é‡)ä¸Šè·å¾—è‰¯å¥½çš„SERæ€§èƒ½ï¼ˆä½œè€…çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œåç»­æ¨¡å‹è®¾è®¡å’Œå®ç°å‡ä»¥æ­¤ä¸ºåŸºç¡€ï¼‰ã€‚ æ¢è¨€ä¹‹ï¼Œæ–‡ç« çš„motivationä¸ºï¼šé€šè¿‡åˆ©ç”¨ç›¸å…³é¢†åŸŸçš„å¤§è§„æ¨¡æ•°æ®ï¼ˆè¯´è¯äººè¯†åˆ«æ•°æ®ï¼‰ï¼Œå…‹æœç°æœ‰æ ‡ç­¾SERæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚\nâ€ƒæœ¬æ–‡çš„ä¸»è¦å·¥ä½œåŒ…æ‹¬ï¼š\n ç”¨åŒç©ºé—´æŸå¤±ï¼ˆdual space lossï¼‰æ¥åˆ†è§£æƒ…æ„Ÿç›¸å…³ ($R^{+}$) å’Œæƒ…æ„Ÿæ— å…³ ($R^{-}$) ç©ºé—´ï¼›ï¼ˆè¿™ä¸¤ä¸ªç©ºé—´çš„è·å–æ–¹æ³•å€¼å¾—æ³¨æ„ï¼Œå°¤å…¶æ˜¯instanceæ­£åˆ™åŒ–ä¹‹åçš„å·®æ“ä½œï¼Œéœ€è¦æ›´æ·±å…¥çš„ç†è§£ï¼‰ é€šè¿‡æ—¶é¢‘æ³¨æ„åŠ›æœºåˆ¶æ¥å¼•å…¥è¿œç¨‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆlong-range contextual informationï¼‰; ä¸å…ˆå‰åŸºäºé£æ ¼åˆ†è§£ï¼ˆstyle disentangleï¼‰çš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç›´æ¥åº”ç”¨åˆ°å‰ç«¯ç‰¹å¾æå–å™¨ä¸Šï¼ˆä¸»è¦æŒ‡å…¶å®ƒä»»åŠ¡çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼‰ã€‚  â€ƒå®éªŒé‡‡ç”¨çš„æ•°æ®é›†æ˜¯IEMOCAPã€‚\n5. Neural Architecture Search For Speech Emotion Recognition  ä½œè€…å•ä½: é¦™æ¸¯ä¸­æ–‡å¤§å­¦; è®ºæ–‡é“¾æ¥\n â€ƒæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºNeural Architecture Search (NAS) çš„ç½‘ç»œç»“æ„æœç´¢ç®—æ³•ï¼Œç§°ä¸ºï¼šç»Ÿä¸€è·¯å¾„ä¸¢å¼ƒç­–ç•¥ (uniform path dropout strategy)ï¼Œä¸»è¦å°±æ˜¯å°†NASåŸæœ‰çš„uniform path samplingæ›¿æ¢ä¸ºdropoutã€‚\nâ€ƒæ‰€ç”¨æ•°æ®é›†ä¸ºIEMOCAPã€‚\n6. Climate and Weather: Inspecting Depression Detection Via Emotion Recognition  ä½œè€…å•ä½: å‰‘æ¡¥å¤§å­¦; è®ºæ–‡é“¾æ¥\n â€ƒæƒ…ç»ªå’ŒæŠ‘éƒä¹‹é—´å­˜åœ¨ç€ç´§å¯†çš„å…³ç³»ï¼Œå› æ­¤ä½œè€…å°†ç”¨äºæƒ…ç»ªè¯†åˆ«çš„æ¨¡å‹è¿ç§»åˆ°æŠ‘éƒ-å¥åº·çŠ¶æ€çš„äºŒåˆ†ç±»æ£€æµ‹ä¸­ã€‚ä½œè€…å®éªŒåå‘ç°ï¼Œæƒ…ç»ªå’ŒæŠ‘éƒçš„å…³ç³»ç±»ä¼¼äºå¤©æ°”å’Œæ°”å€™ï¼Œä¹Ÿå°±æœ‰äº†é¢˜ç›®ä¸­çš„Climateå’ŒWeatherã€‚\nâ€ƒæ‰€ç”¨æƒ…ç»ªè¯†åˆ«æ•°æ®é›†ä¸ºï¼šIEMOCAP/MOSEIï¼›æŠ‘éƒæ£€æµ‹æ•°æ®é›†ä¸ºï¼šDAIC-WOZã€‚\n7. A Commonsense Knowledge Enhanced Network with Retrospective Loss for Emotion Recognition in Spoken Dialog  ä½œè€…å•ä½: å“ˆå·¥å¤§; è®ºæ–‡é“¾æ¥\n â€ƒæœ¬æ–‡ç ”ç©¶åŠ¨æœºåŒ…æ‹¬ï¼š1) ç°æœ‰å¯¹è¯æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›† (Emotion Recognition in Spoken Dialog, ERSD) è§„æ¨¡éƒ½å¾ˆå°ï¼Œé™åˆ¶äº†æ¨¡å‹èƒ½åŠ›ï¼›2) å¯¹è¯ä¸­è¡¨è¾¾çš„æƒ…æ„Ÿä¸å…ˆå‰çš„ç»éªŒæœ‰å…³ï¼Œè¿™ç§ç»éªŒå¯ä»¥æ˜¯å¸¸è¯† (commonsense knowledge, Case #1 in Fig.1)ï¼Œä¹Ÿå¯ä»¥æ˜¯æ¨¡å‹å†å²çš„åˆ¤æ–­ (model historical judgement, Case #2 in Fig.1)ã€‚\nâ€ƒæœ¬æ–‡æå‡ºäº†CKE-Netæ¥ 1ï¼‰å±‚æ¬¡åŒ–åœ°è¿›è¡Œå¯¹è¯å»ºæ¨¡â€”â€”ä¸åŒå±‚ç‰¹å¾çš„concateï¼›2ï¼‰é›†æˆå¤–éƒ¨çŸ¥è¯† (external knowledge integration)ï¼›3ï¼‰å›æº¯å†å²çŠ¶æ€ (historical state retrospect)ã€‚å…¶ä¸­ï¼Œ Knowledge Enhancement Moduleç”¨çš„å¤–éƒ¨çŸ¥è¯†åº“æ˜¯ConceptNetï¼ŒçŸ¥è¯†è¡¨ç¤ºæ–¹å¼ä¸ºå››å…ƒç»„\u0026lt;concept1, relation, concept2, confidence\u0026gt;ï¼Œæœ¬æ–‡æ²¡æœ‰ä½¿ç”¨relationå±æ€§ã€‚\nâ€ƒæ‰€ç”¨æ•°æ®é›†ä¸ºIEMOCAPå’ŒMELD\n8. Towards Transferable Speech Emotion Representation: on Loss Functions for Cross-lingual Latent Representations 9. Is Cross-attention Preferable to Self-attention for Multi-modal Emotion Recognition? 10. Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach 11. Not All Features Are Equal: Selection of Robust Features for Speech Emotion Recognition in Noisy Environments 12. Domain-invariant Feature Learning for Cross Corpus Speech Emotion Recognition 13. Speech Emotion Recognition with Co-attention Based Multi-level Acoustic Information 14. Multi-lingual Multi-task Speech Emotion Recognition Using Wav2vec 2.0 15. Selective Multi-task Learning for Speech Emotion Recognition Using Corpora of Different Styles 16. Enhancing Privacy Through Domain Adaptive Noise Injection for Speech Emotion Recognition 17. Confidence Estimation for Speech Emotion Recognition Based on the Relationship Between Emotion Categories and Primitives 18. Light-sernet: a Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition 19. Key-sparse Transformer for Multimodal Speech Emotion Recognition 20. Speaker Normalization for Self-supervised Speech Emotion Recognition ","date":1656892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657146000,"objectID":"1ad35cb65a76b24887d1136c6d558a1a","permalink":"https://2oil.top/en/post/2022-07-04/","publishdate":"2022-07-04T00:00:00Z","relpermalink":"/en/post/2022-07-04/","section":"post","summary":"ICASSP 2022 è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç›¸å…³è®ºæ–‡æ•´ç†ï¼ˆspeech emotion recognition)","tags":["å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ","è¯­éŸ³æƒ…æ„Ÿåˆ†æ"],"title":"è®ºæ–‡é˜…è¯»â€”â€”å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆä¸‰ï¼‰","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ","è¯­éŸ³æƒ…æ„Ÿåˆ†æ"],"content":"1.1 åŠ¨æœº 1.1.1 emotionçš„å®šä¹‰  ç¦»æ•£å‹ï¼šanger, disgust, fear, happiness, sadness, surpriseå…­ç§åŸºæœ¬ç±»å‹ï¼Œå¯è¿›ä¸€æ­¥æ‹“å±•åˆ°27ç§ [1]ã€‚ è¿ç»­å‹ï¼šActivation (å”¤é†’åº¦), Valence (è­¦è§‰åº¦), Dominance (å—æ”¯é…ç¨‹åº¦) (AVD)ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ã€‚   æƒ…ç»ªçš„å®šä¹‰æœ‰å¾ˆå¤šï¼Œä»¥ä¸‹æ˜¯ä¸¤ç§æ–¹å¼\n æƒ…æ„Ÿæ²™æ¼ æƒ…æ„Ÿæ²™æ¼[3]: Sensitivity, Aptitude, Attention and Pleasantnesså››ç§å±æ€§ä¸Sentiment Polarityä¹‹é—´çš„å…³ç³» æƒ…ç»ªè½®ï¼Œä»¥ä¸‹å‚è€ƒè‡ª[4] ç¦»æ•£æƒ…ç»ª åœ¨æƒ…ç»ªè½®åœ†é”¥ä½“åº•éƒ¨çš„æƒ…ç»ªæ›´å¼ºçƒˆï¼Œå‘ä¸Šéšç€å¼ºåº¦çš„é™ä½ï¼Œå®ƒä»¬æ›´éš¾è¢«åŒºåˆ†å¼€ã€‚\nè¿ç»­ç©ºé—´æƒ…ç»ª  Valenceï¼šæ¶ˆæï¼ˆä¼¤å¿ƒï¼‰â†’ç§¯æï¼ˆé«˜å…´ï¼‰ Arousalï¼šå¹³é™â†’æ¿€åŠ¨ Dominanceï¼šå—/è¢«æ”¯é…â†’åœ¨æ§åˆ¶ä¸­ åœ¨Valenceä¸Arousalç»´åº¦ä¸Šï¼Œè¿ç»­æƒ…ç»ªè¡¨ç¤ºä¸ç¦»æ•£æƒ…ç»ªç±»åˆ«ä¹‹é—´çš„å…³ç³»  1.2.1 å‡è®¾ ä½œè€…è®¤ä¸ºè¯­éŸ³emotionçš„Valenceç»´ä¸æ–‡æœ¬sentimentç›¸å…³ï¼Œå¹¶é€šè¿‡IEMOCAPæ•°æ®é›†è¿›è¡Œäº†éªŒè¯ï¼Œå¦‚è¡¨1ã€‚\n negative text-sentimentå¤šä¸negative speech (sad, anger, frustrated) ç›¸å…³ positive text-sentimentå¤šä¸positive speech (happy) ç›¸å…³ å°†Sad, Frustrated, Angeræ˜ å°„ä¸ºä¸€ç±»åï¼Œè®¡ç®—å¾—åˆ°çš„Spearmanç›¸å…³ç³»æ•°$\\rho$ä¸º0.22ï¼Œä¸ºæ­£ç›¸å…³ ï¼ˆ-1 $\\leq$ $\\rho$ $\\leq$ 1ï¼Œ$\\pm1$æ—¶ä¸ºå®Œå…¨ç›¸å…³ã€‚å‚è€ƒPearsonç›¸å…³æ€§ç³»æ•°è¡¡é‡çš„æ ‡å‡†ï¼Œ0.22å±äºå¼±æ­£ç›¸å…³ï¼‰  1.2 æ¨¡å‹æ¶æ„ 1.2.1 é¢„è®­ç»ƒéƒ¨åˆ† åŒ…å«è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œç›®æ ‡å‡½æ•°ä¸ºï¼š$L_{global} = L_{ASR} + \\lambda L_{sentiment}$ã€‚ä½œè€…åœ¨å®éªŒä¸­å°†$\\lambda$è®¾ç½®æˆäº†200ã€‚ 1.2.2 å¾®è°ƒéƒ¨åˆ† ç›®æ ‡å‡½æ•°ä¸ºï¼š $CCC(y, \\hat y) = \\frac{2Cov(y, \\hat y)}{\\sigma_{y}^{2} + \\sigma_{\\hat y}^{2} + (\\mu_{y} - \\mu_{\\hat y})^{2}}$\n$L_{CCC} = - \\frac{1}{3}(CCC_{A} + CCC_{V} + CCC_{D})$ 1.3 æ•°æ®é›†  æ–‡ç« ç”¨æ¥åšè¯­éŸ³emotionè¯†åˆ«æ•°æ®é›†æ˜¯MSP-Podcast\n 1.3.1 æ•°æ®æ¥æºå’Œä¼—åŒ…æ ‡æ³¨ 1.3.2 æ ‡æ³¨  åŸºäºå±æ€§çš„æè¿°ç¬¦(Activation, Dominance and Valence) åˆ†ç±»æ ‡ç­¾(anger, happiness, sadness, disgust, surprised, fear, contempt, neutral and other)  1.3.3 æ•°æ®è§„æ¨¡  è¯­æ–™åº“çš„æ”¶é›†æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚1.7ç‰ˆæœ¬æœ‰62140æ®µè¯­éŸ³(çº¦100å°æ—¶)\n    æ•°æ®ç±»åˆ« æ•°ç›® å¤‡æ³¨     Train set 38179    Development set 7538 44 speakers (22 female and 22 male)   Test set 1 12902 60 speakers (30 female and 30 male)   Test set 2 3521 randomly select from 100 podcasts. Segments from these podcasts are not included in any other partition.    1.4 å®éªŒç»“æœ ASRç‰¹å¾æœ‰æ•ˆæ€§ æ··åˆtext-sentimentçš„ASRç‰¹å¾æœ‰æ•ˆæ€§ è¿™é‡Œåœ¨è‡ªå·±çš„å®éªŒç»“æœä¸å¦‚CPCæ—¶ï¼Œä½œè€…è‡ªåœ†å…¶è¯´çš„æ–¹æ³•ï¼ˆå¯¹valenceç»´åº¦æ›´æ„Ÿå…´è¶£ï¼‰ï¼šâ€œHowever, in this equal-weight multi-task emotion training setting, we see that activation and dominance dimension performs relatively weak compare to that of. We view this as an encouraging result as in many applications, valence (positive v.s. negative) is of most interest.â€ å¾®è°ƒæœ‰æ•ˆæ€§ å‚è€ƒæ–‡çŒ® [1] Self-report captures 27 distinct categories of emotion bridged by continuous gradients\n[2] Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings\n[3] SenticNet: A Publicly Available Semantic Resource for Opinion Mining\n[4] æƒ…ç»ªè®¡ç®—â€”â€”â€œæƒ…ç»ªç©ºé—´â€è¡¨è¾¾\n","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656806400,"objectID":"c57039373ab970028ed8aa6aaea35cf3","permalink":"https://2oil.top/en/post/2022-07-03/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/en/post/2022-07-03/","section":"post","summary":"é€šè¿‡å€ŸåŠ©æ–‡æœ¬æ¨¡æ€çš„å¼±æ ‡ç­¾æ¥è®­ç»ƒè¯­éŸ³æƒ…ç»ªç‰¹å¾ï¼Œä»è€Œè¿›è¡Œè¿ç»­ç©ºé—´çš„æƒ…ç»ªè¯†åˆ«ã€‚","tags":["å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ","è¯­éŸ³æƒ…æ„Ÿåˆ†æ"],"title":"è®ºæ–‡é˜…è¯»â€”â€”å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆäºŒï¼‰","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["ç‚¼ä¸¹æŠ€å·§","å¯¹æŠ—è®­ç»ƒ"],"content":"FGM class FGM: \u0026quot;\u0026quot;\u0026quot; å‚è€ƒè‡ª: https://blog.csdn.net/qq_40176087/article/details/121512229 FGSMçš„æ›´æ–°å…¬å¼ä¸º: eplison * torch.sign(param.grad) \u0026quot;\u0026quot;\u0026quot; def __init__(self, model, eps=1.) -\u0026gt; None: self.model = model self.eps = eps self.backup = {} # only attack word embedding def attack(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm and not torch.isnan(norm): r_at = self.eps * param.grad / norm param.data.add_(r_at) def restore(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: assert name in self.backup param.data = self.backup[name] self.backup = {}  FGSM class FGSM: \u0026quot;\u0026quot;\u0026quot; å‚è€ƒè‡ª: https://blog.csdn.net/qq_40176087/article/details/121512229 FGSMçš„æ›´æ–°å…¬å¼ä¸º: eplison * torch.sign(param.grad) \u0026quot;\u0026quot;\u0026quot; def __init__(self, model, eps=1.) -\u0026gt; None: self.model = model self.eps = eps self.backup = {} # only attack word embedding def attack(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm and not torch.isnan(norm): r_at = self.eps * torch.sign(param.grad) param.data.add_(r_at) def restore(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: assert name in self.backup param.data = self.backup[name] self.backup = {}  PGD class PGD: def __init__(self, model, eps=1., alpha=0.3) -\u0026gt; None: self.model = model self.eps = eps self.alpha = alpha self.emb_backup = {} self.grad_backup = {} def attack(self, embedding_name=\u0026quot;word_embeddings\u0026quot;, is_first_attack=False): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: if is_first_attack: self.emb_backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_at = self.alpha * param.data / norm param.data.add_(r_at) param.data = self.project(name, param.data) def restore(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: assert name in self.emb_backup param.data = self.emb_backup[name] self.emb_backup = {} def project(self, param_name, param_data): r = param_data - self.emb_backup[param_name] if torch.norm(r) \u0026gt; self.eps: r = self.eps * r / torch.norm(r) return self.emb_backup[param_name] + r def backup_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad and param.grad is not None: self.grad_backup[name] = param.grad.clone() def restore_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad and param.grad is not None: param.grad = self.grad_backup[name]  å®éªŒæ•ˆæœ åœ¨å¾®åšæƒ…ç»ªè¯†åˆ«ä»»åŠ¡[2]ä¸­ï¼Œæœ‰å¾®å¼±æå‡ã€‚\n   method performance     without adv 96.99   with fgm 97.12   $\\Delta$ 0.13   with fgsm è®­ç»ƒå¤±è´¥ï¼ŒéªŒè¯é›†å‡†ç¡®ç‡ä¸‹é™åˆ°52ï¼Œè€ƒè™‘æ¢¯åº¦é—®é¢˜    å‚è€ƒæ–‡çŒ® [1] å¯¹æŠ—è®­ç»ƒfgmã€fgsmå’ŒpgdåŸç†å’Œæºç åˆ†æ\n[2] ç–«æƒ…å¾®åšæƒ…ç»ªè¯†åˆ«æŒ‘æˆ˜èµ›\n","date":1656460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656460800,"objectID":"dc10412eb620e43e59e8a787780daf32","permalink":"https://2oil.top/en/post/2022-06-29/","publishdate":"2022-06-29T00:00:00Z","relpermalink":"/en/post/2022-06-29/","section":"post","summary":"é€šè¿‡å¼•å…¥æ¢¯åº¦å™ªå£°æ¥å¯¹æ¨¡å‹æ­£åˆ™åŒ–ï¼Œä»è€Œå¢åŠ æ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ï¼Œä»£ä»·æ˜¯è®­ç»ƒé€Ÿåº¦æˆå€ä¸‹é™ã€‚å…·ä½“å“ªä¸€ç§å¯¹æŠ—è®­ç»ƒæ–¹å¼æœ‰æ•ˆï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡ŒéªŒè¯ã€‚","tags":["ç‚¼ä¸¹æŠ€å·§","å¯¹æŠ—è®­ç»ƒ","FGM","FGSM","PGD"],"title":"ç‚¼ä¸¹æŠ€å·§-å¯¹æŠ—è®­ç»ƒ","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["ç‚¼ä¸¹æŠ€å·§"],"content":"åŸç†  æ¥è‡ªäºå‚è€ƒæ–‡çŒ®[1]\n åŸºæœ¬å‡è®¾ï¼šæ¨¡å‹æƒé‡åœ¨æœ€åçš„$n$æ­¥å†…ï¼Œä¼šåœ¨å®é™…çš„æœ€ä¼˜ç‚¹å¤„æŠ–åŠ¨ï¼Œå› æ­¤å–æœ€å$n$æ­¥çš„å¹³å‡ï¼Œèƒ½ä½¿æ¨¡å‹æ›´åŠ é²æ£’ã€‚\næƒé‡å‚æ•°æ›´æ–°å…¬å¼ï¼š$v_{t} = \\beta * v_{t-1} + (1 - \\beta) * v_{t}$\nä»£ç   æ¥è‡ªäºå‚è€ƒæ–‡çŒ®[1]ï¼Œæ³¨æ„åªåœ¨éªŒè¯å’Œæµ‹è¯•æ—¶ä½¿ç”¨emaçš„å¹³å‡å‚æ•°ï¼Œè®­ç»ƒæ—¶ä¸ç”¨ã€‚\n class EMA: def __init__(self, model, decay): self.model = model self.decay = decay self.shadow = {} self.backup = {} def register(self): for name, param in self.model.named_parameters(): if param.requires_grad: self.shadow[name] = param.data.clone() def update(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name] self.shadow[name] = new_average.clone() def apply_shadow(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow self.backup[name] = param.data param.data = self.shadow[name] def restore(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.backup param.data = self.backup[name] self.backup = {}  å®éªŒæ•ˆæœ åœ¨å¾®åšæƒ…ç»ªè¯†åˆ«ä»»åŠ¡[2]ä¸­ï¼Œæœ‰å¾®å¼±æå‡ã€‚\n   method performance     without ema 96.83   with ema 96.99   $\\Delta$ 0.16    å‚è€ƒæ–‡çŒ® [1] ã€ç‚¼ä¸¹æŠ€å·§ã€‘æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰çš„åŸç†åŠPyTorchå®ç°\n[2] ç–«æƒ…å¾®åšæƒ…ç»ªè¯†åˆ«æŒ‘æˆ˜èµ›\n","date":1656288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656288000,"objectID":"7a1dd6fea04141125cfd6cd8d0ee3960","permalink":"https://2oil.top/en/post/2022-06-27/","publishdate":"2022-06-27T00:00:00Z","relpermalink":"/en/post/2022-06-27/","section":"post","summary":"é€šè¿‡EMAæ–¹æ³•å¯¹æ¨¡å‹çš„å‚æ•°åšæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œä»è€Œæé«˜æµ‹è¯•æŒ‡æ ‡å¹¶å¢åŠ æ¨¡å‹é²æ£’æ€§ã€‚","tags":["ç‚¼ä¸¹æŠ€å·§","EMA"],"title":"ç‚¼ä¸¹æŠ€å·§-EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰","type":"post"},{"authors":["Sheng Zhang","Min Chen","Jincai Chen","Yuan-Fang Li","Yiling Wu","Minglei Li","Chuanbo Zhu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"5f4c4112b457c2ccfea2f1cbe9c42012","permalink":"https://2oil.top/en/publication/kbs2021/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/en/publication/kbs2021/","section":"publication","summary":"To alleviate the above issue, we propose a new architecture that combines cross-modal knowledge transfer from visual to audio modality into our semi-supervised learning method with consistency regularization. We posit that introducing visual emotional knowledge by the cross-modal transfer method can increase the diversity and accuracy of pseudo-labels and improve the robustness of the model. To combine knowledge from cross-modal transfer and semi-supervised learning, we design two fusion algorithms, i.e. weighted fusion and consistent \u0026 random.","tags":[],"title":"Combining cross-modal knowledge transfer and semi-supervised learning for speech emotion recognition","type":"publication"},{"authors":["Chuanbo Zhu"],"categories":["åŸºç¡€çŸ¥è¯†","åˆ†ç±»æŒ‡æ ‡"],"content":"   real = 1 real = 0     predict = 1(TP) predict = 1(FP)   predict = 0(FN) predict = 0(TN)     TP: true positive,æ­£ç±»é¢„æµ‹æ­£ç¡® FN: false negative,æ­£ç±»é¢„æµ‹é”™è¯¯ FP: false positive,è´Ÿç±»é¢„æµ‹é”™è¯¯ TN: true negative,è´Ÿç±»é¢„æµ‹æ­£ç¡®  æ€»çš„æ ·æœ¬ä¸ªæ•°ä¸ºï¼šTP + FN + FP + TN\nå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ $${accuracy = \\frac{TP + TN}{TP + FN + FP + TN}}$$\næŸ¥å‡†ç‡æˆ–ç²¾åº¦ï¼ˆprecisionï¼‰ $$ precision = \\frac{TP}{TP + FP} $$\næŸ¥å…¨ç‡æˆ–å¬å›ç‡ï¼ˆrecallï¼‰ $$ recall = \\frac{TP}{TP + FN} $$\n å¦‚ä½•ç†è§£Precision/Recall å‡è®¾100ç™Œç—‡è®­ç»ƒé›†ä¸­ï¼Œåªæœ‰1ä¾‹ä¸ºç™Œç—‡ã€‚å¦‚æœæ¨¡å‹æ°¸è¿œé¢„æµ‹y=0ï¼Œåˆ™æ¨¡å‹çš„Precision=99/100ï¼Œå¾ˆé«˜ã€‚ä½†Recall=0/1=0,éå¸¸ä½ã€‚ æ‰€ä»¥å•çº¯ç”¨Precisionæ¥è¯„ä»·æ¨¡å‹æ˜¯ä¸å®Œæ•´çš„ï¼Œè¯„ä»·æ¨¡å‹æ—¶å¿…é¡»ç”¨Precision/Recallä¸¤ä¸ªå€¼ã€‚\n F1æŒ‡æ ‡  ç»¼åˆäº†æŸ¥å‡†ç‡ï¼ˆprecisionï¼‰å’ŒæŸ¥å…¨ç‡ï¼ˆrecallï¼‰çš„æŒ‡æ ‡\n $$ F1 = \\frac{2 * precision * recall}{precision + recall} $$\nä¸Šè¿°å››ç§æŒ‡æ ‡å¸¸ç”¨äºäºŒåˆ†ç±»é—®é¢˜ï¼Œå¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œå¸¸æœ‰ä»¥ä¸‹æŒ‡æ ‡ï¼š\nMacro Average  ç»¼åˆè¿‡åè¿›è¡Œå–å¹³å‡å€¼çš„è®¡ç®—, åŒ…æ‹¬:\n Macro-accuracy\nMacro-precision\nMacro-recall\nMacro-F1\n  $$ precision_i = \\frac{TP}{TP + FP}, \\ for \\ i \u0026lt; number_{type} $$ $$ recall_i = \\frac{TP}{TP + FN}, \\ for \\ i \u0026lt; number_{type} $$ $$ f_i = \\frac{2 * precision * recall}{precision + reacll}, \\ for \\ i \u0026lt; number_{type} $$\n æœ€ç»ˆçš„æ±‚è§£ç»“æœ\n $$ precision = \\frac{\\sum_i{precision_i}}{number_{type}} $$ $$ recall = \\frac{\\sum_i{recall_i}}{number_{type}} $$ $$ f = \\frac{\\sum_i{f_i}}{number_{type}} $$\n","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"739d95c5332acf10df6af237cbb9788b","permalink":"https://2oil.top/en/post/2020-07-20/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/en/post/2020-07-20/","section":"post","summary":"Welcome ğŸ‘‹ We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["åŸºç¡€çŸ¥è¯†","åˆ†ç±»æŒ‡æ ‡"],"title":"åˆ†ç±»é—®é¢˜ä¸­çš„ç›¸å…³æŒ‡æ ‡","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["æ–‡çŒ®é˜…è¯»","å¤šæ¨¡æ€æ™ºèƒ½","å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ"],"content":"æ–‡ç« ä¸€ï¼šMemory Fusion Network for Multi-view Sequential Learning[@Zadeh2018]  2018å¹´AAAIä¼šè®®æ–‡ç« ï¼Œä¸»è¦å·¥ä½œæ˜¯æ„é€ äº†é’ˆå¯¹å¤šæ¨¡æ€æ•°æ®çš„MFN\n Basic concepts å¤šæ¨¡æ€çš„åºåˆ—å­¦ä¹ åŒ…æ‹¬ä¸¤ä¸ªè¿‡ç¨‹\n view-specific interactions that handle only one view cross-view interactions which are defined across different views and need to handle multi-views  so, the modeling of view-specific interactions and cross-view interactions is the core question of multi-view sequential learning[@Zadeh2018]\næ–‡ç« å°†å¤šæ¨¡æ€å­¦ä¹ åˆ†ä¸ºä¸‰ç±»ï¼ŒåŸºæœ¬æ€è·¯æ˜¯å°†ä¸åŒæ¨¡æ€ä¸‹çš„ç‰¹å¾å‘é‡æ˜ å°„åˆ°åŒä¸€ä¸ªç‰¹å¾å­ç©ºé—´\n the first category of models have relied on concatenation of all multiple views into a single view to simplify the learning setting   æ€è·¯ï¼šé€šè¿‡å°†ä¸åŒæ¨¡æ€æ˜ å°„åˆ°æŸä¸ªå•ä¸€æ¨¡æ€æ¥è·å¾—inputå±‚æ¬¡çš„çº§è”ï¼Œå†é€å…¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œå¾—åˆ°ç‰¹å¾å‘é‡\n**é—®é¢˜ï¼š**ç®€å•çš„çº§è”å¿½è§†äº†ä¸åŒæ¨¡æ€çš„æ•°æ®æœ¬æ¥å°±æœ‰ä¸“å±ç‰¹å¾\n the second category of models introduce multi-view variants to the structured learning approaches of the first category   æ€è·¯ï¼šå¯¹ç¬¬ä¸€ç§çš„æ”¹è¿›ï¼Œå•ç‹¬å­¦ä¹ æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾å‘é‡ï¼Œæ˜¯ç‰¹å¾å±‚æ¬¡ä¸Šçš„çº§è”\né—®é¢˜ï¼š ç‰¹å¾å‘é‡çš„çº§è”ï¼Œå¦‚$${v}, {a}, {l} -\u0026gt; {v, a, l}$$å¹¶æ²¡æœ‰åšåˆ°çœŸæ­£çš„æ•°æ®èåˆ\n the third category of models rely on collapsing the time dimension from sequences by learning a temporal representation for each of the different views.   æ€è·¯ï¼š æŒ‰ç…§æ—¶é—´ç»´åº¦èåˆå¤šæ¨¡æ€çš„æ•°æ®ï¼Œèåˆåçš„ç‰¹å¾å‘é‡æ˜¯å‡ ä¸ªæ¨¡æ€çš„å¹³å‡ï¼Œæ˜¯ç‰¹å¾å±‚æ¬¡ä¸Šçš„çº§è”ï¼Œæ˜¯modality fusionï¼Œæˆ–è€…é€šè¿‡votingçš„æ–¹å¼è·å–ç»“æœ\n my question: I don\u0026rsquo;t think that combining models by voting is a fusion method which should do fusion before generating decision, but the author classified it to the thrid category.\n   é—®é¢˜ï¼š å¹³å‡çš„æ–¹å¼æ©ç›–äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚æ€§\n Motivation Innovation DataSets Memory Fusion Network (MFN) ç”±ä¸‰éƒ¨åˆ†æ„æˆï¼š\n the Systems of LSTMs for view-specific interactions a special attention mechanism for cross-view interactions(DMAN) Multi-view Gated Memory for summarization  æ–‡ç« äºŒï¼šTensor Fusion Network for Multimodal Sentiment Analysis. è¿™æ˜¯æ™®é€šçš„æ–‡å­—\nè®ºæ–‡ç¬”è®°å‰ä¼  ","date":1594857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594857600,"objectID":"a88cb98fbe5de16d4d982c2df7486a0b","permalink":"https://2oil.top/en/post/2020-07-16/","publishdate":"2020-07-16T00:00:00Z","relpermalink":"/en/post/2020-07-16/","section":"post","summary":"Two papers, \"Memory Fusion Network for Multi-view Sequential Learning\" and \"Tensor Fusion Network for Multimodal Sentiment Analysis\"","tags":["TFN","MFN","å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ"],"title":"è®ºæ–‡é˜…è¯»â€”â€”å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆä¸€ï¼‰","type":"post"},{"authors":null,"categories":null,"content":"é‡‡ç”¨ Flask æ¡†æ¶å®ç°ï¼ŒåŒ…æ‹¬å­¦ç”Ÿä¿¡æ¯ç®¡ç†ã€è¯•é¢˜ä¿¡æ¯ç®¡ ç†ã€è¯•å·ç”Ÿæˆä¸è¯•é¢˜å¯¼å‡ºç­‰æ¨¡å—\n","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"c6ef40d6c0b8c23618271f3aac192a74","permalink":"https://2oil.top/en/project/retest/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/en/project/retest/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Web"],"title":"åä¸­å†œä¸šå¤§å­¦é£Ÿå“ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ç ”ç©¶ç”Ÿå¤è¯•ç³»ç»Ÿ","type":"project"},{"authors":["Chuanbo Zhu","å³æ©é”"],"categories":["Demo","æ•™ç¨‹"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  ğŸ‘‰ Create a new site ğŸ“š Personalize your site ğŸ’¬ Chat with the Wowchemy community or Hugo community ğŸ¦ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy ğŸ’¡ Request a feature or report a bug for Wowchemy â¬†ï¸ Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\nâ¤ï¸ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future â¤ï¸ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ğŸ¦„âœ¨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://2oil.top/en/post/getting-started/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/en/post/getting-started/","section":"post","summary":"Welcome ğŸ‘‹ We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","å¼€æº"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"è¯¥é¡¹ç›®æ¨¡ä»¿æ—©æœŸ Linux ç‰ˆæœ¬æ‰€é‡‡ç”¨çš„çš„ Ext2 æ–‡ä»¶ç³»ç»Ÿï¼Œé€šè¿‡ 8.1M çš„æ–‡ä»¶æ¨¡æ‹Ÿç£ç›˜ç©ºé—´ï¼Œå®ç°æ–‡ä»¶ç³»ç»Ÿ å¯¹ç£ç›˜ç®¡ç†ã€å†…å­˜ç®¡ç†çš„ä»¿çœŸã€‚\nç£ç›˜ç®¡ç†ä¸­å°†æ¯ä¸€ä¸ªç‰©ç†å—å®šä¹‰ä¸º 1K å¤§å°ï¼Œé€»è¾‘å—å®šä¹‰ä¸º 1K å¤§å°ã€‚æŒ‰ç…§ Ext2 æ–‡ä»¶ç³»ç»Ÿçš„æ ¼å¼ï¼Œå°†ç³» ç»Ÿåˆ’åˆ†ä¸º 8 ä¸ªç»„ï¼Œæ¯ä¸ªç»„å¤§å°ä¸º 1024Kï¼ŒåˆåŒ…å«è¶…çº§å—ã€ç»„æè¿°ç¬¦ã€å—ä½å›¾ã€é€»è¾‘ä½å›¾ã€å†…å­˜ç´¢å¼•èŠ‚ç‚¹å’Œ æ•°æ®å—ã€‚æŒ‰ç…§åˆ›å»ºæ–‡ä»¶ã€åˆ›å»ºç›®å½•ã€åˆ é™¤æ–‡ä»¶ã€åˆ é™¤ç›®å½•ã€æ‰“å¼€æ–‡ä»¶ã€è¯»æ–‡ä»¶ã€å†™æ–‡ä»¶çš„ç®—æ³•åˆ†åˆ«å®ç°ç›¸å…³ å‡½æ•°ã€‚\nå†…å­˜ç®¡ç†ä¸­å»ºç«‹å†…å­˜ç´¢å¼•èŠ‚ç‚¹ã€ç³»ç»Ÿæ‰“å¼€æ–‡ä»¶è¡¨ã€ç”¨æˆ·æ‰“å¼€æ–‡ä»¶è¡¨ã€è¿›ç¨‹æ•°æ®ç»“æ„ã€‚æ¨¡æ‹Ÿè¿›ç¨‹æ“ä½œæ–‡ä»¶æ—¶ï¼Œ é€šè¿‡å†…å­˜ç´¢å¼•èŠ‚ç‚¹ä¿¡æ¯ï¼Œè°ƒç”¨åº•å±‚æ–‡ä»¶æ“ä½œå‡½æ•°ï¼Œä¾æ¬¡å¡«å……ç³»ç»Ÿæ‰“å¼€æ–‡ä»¶è¡¨ï¼Œç”¨æˆ·æ‰“å¼€æ–‡ä»¶è¡¨æ¥å®ç°è¿›ç¨‹ å¯¹æ–‡ä»¶çš„æ“ä½œã€‚\n","date":1530057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530057600,"objectID":"d80e40007e8f9060a31bc1a59ea6bb61","permalink":"https://2oil.top/en/project/ext2-fs/","publishdate":"2018-06-27T00:00:00Z","relpermalink":"/en/project/ext2-fs/","section":"project","summary":"ä¸ªäººé¡¹ç›®","tags":["Other"],"title":"Ext2 æ–‡ä»¶ç³»ç»Ÿä»¿çœŸ","type":"project"},{"authors":null,"categories":null,"content":"è¯¥å¹³å°åŒ…æ‹¬åç«¯å’Œå‰ç«¯ä¸¤ä¸ªéƒ¨åˆ†ï¼Œé‡‡ç”¨ Spring boot MyBatis + Vue æ¶æ„ï¼Œæ•°æ®åº“ä½¿ç”¨çš„æ˜¯ MySQLã€‚\nå‰æœŸï¼Œè´Ÿè´£ç™»å½•æ¨¡å—å’Œæƒé™æ¨¡å—ï¼Œç™»å½•æ¨¡å—ä½¿ç”¨å†…å­˜æ•°æ®åº“ Redis æ¥ä¿å­˜ç™»å½•çŠ¶æ€å¹¶æé«˜è®¿é—®é€Ÿåº¦; æƒé™ æ¨¡å—é‡‡ç”¨è§’è‰²æƒé™ç®¡ç†æ¨¡å¼ï¼Œå°†ç”¨æˆ·ã€è§’è‰²ã€æƒé™è¿›è¡Œå…³è”ï¼Œå®ç°å¯¹æ€»å…¬å¸ã€çœã€å¸‚ã€å¿å››å±‚ä»£ç†çš„ç®¡ç†ã€‚\nåæœŸï¼Œè´Ÿè´£é¡¹ç›®çš„éœ€æ±‚è¿­ä»£ç®¡ç†ã€æ¨¡å—è®¾è®¡ã€å¹³å°ä¸Šçº¿å’Œç‰ˆæœ¬ç»´æŠ¤ã€‚\n","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"d9c7c5c478bf2d5ec136125faf2a21e3","permalink":"https://2oil.top/en/project/springboot-vue/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/en/project/springboot-vue/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Web"],"title":"åŸºäº Spring Boot ä¸ Vue çš„å±±ç¾Šç”Ÿäº§ç®¡ç†è¿½æº¯å¹³å°","type":"project"},{"authors":null,"categories":null,"content":"é‡‡ç”¨åŸºäº JSP+JavaBean+Servlet çš„ MVC ä¸‰å±‚è®¾è®¡æ¨¡å¼ï¼Œå°†ä¸¤ä¸ªä¸åŒçš„å­¦ç”Ÿæ•°æ®åº“è¿› è¡Œæ•°æ®é›†æˆï¼Œä»è€Œè¾¾åˆ°ç³»ç»Ÿç»¼åˆåº”ç”¨æ•ˆæœã€‚\n","date":1514332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514332800,"objectID":"be53c3335c0ff08119722826af3b4360","permalink":"https://2oil.top/en/project/student-system/","publishdate":"2017-12-27T00:00:00Z","relpermalink":"/en/project/student-system/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Web"],"title":"å­¦ç”Ÿä¿¡æ¯ç®¡ç†ç³»ç»Ÿ","type":"project"},{"authors":null,"categories":null,"content":"","date":1498521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498521600,"objectID":"9736732209c53abf2eaa0b6af5847057","permalink":"https://2oil.top/en/project/single-cpu/","publishdate":"2017-06-27T00:00:00Z","relpermalink":"/en/project/single-cpu/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Other"],"title":"å•å‘¨æœŸ CPU çš„è®¾è®¡ä¸å®ç°","type":"project"},{"authors":null,"categories":null,"content":"åŒ…æ‹¬å…¬å…±å®éªŒå¹³å°ã€è®¾æ–½è®¾å¤‡ç®¡ç†å¹³å°ã€ä½œç‰©å­¦å®éªŒ\næ•™å­¦ä¸­å¿ƒå®˜ç½‘å’Œä½œç‰©å­¦äº’åŠ¨äº¤æµå¹³å°å››ä¸ªè½¯ä»¶ç³»ç»Ÿçš„åç«¯éƒ¨åˆ†è®¾è®¡ä¸å®ç°ã€‚\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"72458671e408e7f6f26cf71693afd644","permalink":"https://2oil.top/en/project/hzau-zky/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/hzau-zky/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Web"],"title":"åä¸­å†œä¸šå¤§å­¦æ¤ç‰©ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢è‹¥å¹²è½¯ä»¶å¹³å°å¼€å‘","type":"project"},{"authors":null,"categories":null,"content":"ä½œä¸ºç»„é•¿ï¼Œç»„ç»‡ä¸‰ä½åŒå­¦åˆ†æé¡¹ç›®éœ€æ±‚ï¼Œè®¾è®¡é¡¹ç›®æ¶æ„ï¼Œæ˜ç¡® FTP åŸç†ï¼Œç»Ÿç­¹æ•´ä¸ªé¡¹ç›®ã€‚å‰æœŸï¼Œè´Ÿè´£å®ç°æ”¯æŒ ASCII ä¼ è¾“ã€äºŒè¿›åˆ¶ä¼ è¾“ï¼Œä¸»åŠ¨æ–¹å¼ä¼ è¾“ã€è¢«åŠ¨æ–¹å¼ä¼ è¾“ï¼Œæ–­ç‚¹ç»­ä¼ åŠŸèƒ½çš„FTPå®¢æˆ·ç«¯é€»è¾‘ä»£ç ã€‚åæœŸï¼Œè´Ÿè´£æµ‹è¯•å¹¶æ’°å†™æµ‹è¯•åˆ†ææŠ¥å‘Šã€‚\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"28acc337f9d2d58fc5c941a9a70e6fbf","permalink":"https://2oil.top/en/project/java-ftp/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/java-ftp/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Other"],"title":"åŸºäº Java çš„ FTP é¡¹ç›®å¼€å‘","type":"project"},{"authors":null,"categories":null,"content":"è¯¥å¹³å°åˆ©ç”¨ TensorFlow ç‰ˆçš„ Keras åº“ï¼Œæ­å»ºå·ç§¯ç¥ç»ç½‘ç»œï¼Œå°†å‰æœŸè·å–çš„å›¾åƒè¿›è¡Œå‰æœŸé¢„å¤„ç†ï¼Œé‡‡ç”¨äº¤å‰éªŒè¯æ–¹å¼è®­ç»ƒç¥ç»ç½‘ç»œï¼Œæœ€ç»ˆå®ç°ä»å®æ—¶è§†é¢‘æµä¸­è¯†åˆ«äººè„¸ä¿¡æ¯ã€‚\nå‰æœŸï¼Œä¸»è¦è´Ÿè´£ç›¸å…³æ–‡çŒ®çš„æŸ¥è¯¢ä»¥åŠé‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œæ–¹å¼çš„ç¡®å®šã€‚\nåæœŸï¼Œä¸»è¦è´Ÿè´£å›¾åƒé¢„å¤„ç† (ç°åº¦å˜æ¢å¤„ç†ã€å‡è¡¡åŒ–å¤„ç†ã€å¹³æ»‘å¤„ç†)ï¼Œä»¥åŠå·ç§¯ç¥ç»ç½‘ç»œçš„æ„å»ºå’Œè®­ç»ƒã€‚\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"dd81e60e07e512e61051f226e3533751","permalink":"https://2oil.top/en/project/face-recognition/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/face-recognition/","section":"project","summary":"æ ¸å¿ƒæˆå‘˜","tags":["Deep Learning"],"title":"åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„äººè„¸è¯†åˆ«ç³»ç»Ÿ","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://2oil.top/en/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":["Chao Sun","Min Chen","Jialiang Cheng","Han Liang","Chuanbo Zhu","Jincai Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8051153f3b03f1bf4a79b070349aea01","permalink":"https://2oil.top/en/publication/mm2023/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/publication/mm2023/","section":"publication","summary":"Will be updated soon.","tags":[],"title":"SCLAV: Supervised Cross-modal Contrastive Learning for Audio-Visual Coding","type":"publication"}]
