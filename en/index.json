[{"authors":null,"categories":null,"content":"Life is like a train, a train bound for the tomb, there will be a lot of stations on the road, it was difficult to keep accompany to walk. When you get out, even if is not, the grateful, then waved goodbye. When goodbye, never say goodbye.\nLove life, love to study, dare to try, dare to challenge, never give up!\n Download my resumé. -- ","date":1690934400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690934400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Life is like a train, a train bound for the tomb, there will be a lot of stations on the road, it was difficult to keep accompany to walk. When you get out, even if is not, the grateful, then waved goodbye.","tags":null,"title":"Chuanbo Zhu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://2oil.top/en/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Chuanbo Zhu","Min Chen","Sheng Zhang","Chao Sun","Han Liang","Yifan Liu","Jincai Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1690934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690934400,"objectID":"1507621ea8758022de1c4e2bd0df7d30","permalink":"https://2oil.top/en/publication/inffus2023/","publishdate":"2023-08-02T00:00:00Z","relpermalink":"/en/publication/inffus2023/","section":"publication","summary":"Propose a sentiment knowledge-enhanced attention fusion network (SKEAFN); Build an additional knowledge graph to leverage explicit sentiment information; Use a multi-head attention mechanism to model the interactions among modalities; Develop feature-wised attention to adjust the contributions of multiple modalities.","tags":[],"title":"SKEAFN: Sentiment Knowledge Enhanced Attention Fusion Network for multimodal sentiment analysis","type":"publication"},{"authors":["Yifan Liu","Jincai Chen","Ping Lu","Chuanbo Zhu","Yugen Jian","Chao Sun","Han Liang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1685664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685664000,"objectID":"d1e8150e98df6424bcf60630a5c2c6ea","permalink":"https://2oil.top/en/publication/caj2023/","publishdate":"2023-06-02T00:00:00Z","relpermalink":"/en/publication/caj2023/","section":"publication","summary":"An efficient desnowing method named Snowed Autoencoder (SAE) is proposed; A multi-encoder for SAE using snow information with an attention module is proposed; A multi-decoder for SAE with fewer sub-decoders than sub-encoders is proposed; SAE outperformed state-of-the-art on four desnowing datasets.","tags":[],"title":"Snowed autoencoders are efficient snow removers","type":"publication"},{"authors":["Yifan Liu","Jincai Chen","Ping Lu","Chuanbo Zhu","Yugen Jian","Chao Sun","Han Liang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1681948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681948800,"objectID":"78607c35d40ef2128320e44b412a9588","permalink":"https://2oil.top/en/publication/tjsc2023/","publishdate":"2023-04-20T00:00:00Z","relpermalink":"/en/publication/tjsc2023/","section":"publication","summary":"Deep learning-based methods have achieved excellent performance in image-deraining tasks. Unfortunately, most existing deraining methods incorrectly assume a uniform rain streak distribution and a fixed fine-grained level. And this uncertainty of rain streaks will result in the model not being competent at repairing all fine-grained rain streaks. In addition, some existing convolution-based methods extend the receptive field mainly by stacking convolution kernels, which frequently results in inaccurate feature extraction. In this work, we propose momentum-contrast and large-kernel for multi-fine-grained deraining network (MOONLIT). To address the problem that the model is not competent at all fine-grained levels, we use the unsupervised dictionary contrastive learning method to treat different fine-grained rainy images as different degradation tasks. Then, to address the problem of inaccurate feature extraction, we carefully constructed a restoration network based on large-kernel convolution with a larger and more accurate receptive field. In addition, we designed a data enhancement method to weaken features other than rain streaks in order to be better classified for different degradation tasks. Extensive experiments on synthetic and real-world deraining datasets show that the proposed method MOONLIT achieves the state-of-the-art performance on some datasets. Code is available at https://github.com/awhitewhale/moonlit.","tags":[],"title":"MOONLIT: momentum-contrast and large-kernel for multi-fine-grained deraining","type":"publication"},{"authors":["Yifan Liu","Jincai Chen","Ping Lu","Chuanbo Zhu","Yugen Jian","Chao Sun","Han Liang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1677974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677974400,"objectID":"c8d164937f870d35ff4983b8c2bd7b19","permalink":"https://2oil.top/en/publication/displays2023/","publishdate":"2023-03-05T00:00:00Z","relpermalink":"/en/publication/displays2023/","section":"publication","summary":"Multi-scaled feature-fused image dehazing network called MFID-Net is proposed; MFID-Net uses dynamic dehazing convolution and activation to improve image dehazing; The proposed dynamic dehazing convolution utilizes dynamic weight fusion; The proposed dynamic dehazing activation uses input global context encoding function; MFID-Net achieves the state-of-the-art performance on dehazing datasets.","tags":[],"title":"MFID-Net: Multi-scaled feature-fused image dehazing via dynamic weights","type":"publication"},{"authors":["Chuanbo Zhu"],"categories":["多模态情感分析","对话情感识别","深度图卷积网络"],"content":"作者单位: 中国人民大学 金琴团队  [论文链接](https://aclanthology.org/2021.acl-long.440.pdf)  Abstract: Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic re- sponses. However, most works focus on mod- eling speaker and contextual information pri- marily on the textual modality or simply lever- aging multimodal information through fea- ture concatenation. In order to explore a more effective way of utilizing both multi- modal and long-distance contextual informa- tion, we propose a new model based on mul- timodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effec- tively, but also leverage speaker information to model inter-speaker and intra-speaker de- pendency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effec- tiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting. -- 1. ERC任务定义、应用场景、关键挑战 ERC任务是情感对话系统的重要组成部分，旨在自动识别并跟踪对话期间说话者的情绪状态，可广泛用于电子医疗服务、智能客服等众多人机交互领域。其关键技术挑战包括：1）如何对conversation的基本组成单元utterance进行上下文建模；2）如何利用speaker信息；3）如何获取并利用conversation的topic信息（针对conversation中topic变化的情形）；4）如何利用其它模态的信息，如视觉/听觉模态等。\n2. 常用数据集 MELD/IEMOCAP是两个常用的多模态ERC数据集，其统计信息如下所示：\n   数据集 conversation (Train/Valid/Test) utterance (Train/Valid/Test)     MELD 1038/114/280 9989/1109/2610 (13708)   IEMOCAP 100/20/31 4810/1000/1623 (7433)    注意：不同论文列出的样本数并不相同（如ACL2021的MMGCN、CVPR2022的M2FNet、ICASSP2022的CKE-Net，需要根据数据集实际的处理情况进行更改。\nconversation中utterance个数统计\n   数据集 Max Min Mean     MELD 24 1 9.62   IEMOCAP       conversation中文本模态的长度统计\n   数据集 Max Min Mean     MELD 321 1 77.33   IEMOCAP       MELD数据集：包含文本、视觉、听觉三种模态的高质量对齐数据，包含304个不同的speakers，每个conversation都包含三个及三个以上的speakers。所有的utterance都标注了七类emotion (anger, disgust, fear, joy, neutral, sadness, and surprise)和三类sentiment (neutral, positive, and negative)标签。原始数据集已包含Valid和Test。\n效果排行榜：https://paperswithcode.com/sota/emotion-recognition-in-conversation-on-meld\nIEMOCAP数据集：包含文本、视觉、听觉三种模态的近12小时two-way conversations数据，包含10个speakers。所有的utterance都标注了六类emotion (happy, sad, neutral, angry, excited, and frustrated)标签。原始数据集无Valid，CVPR2022的M2FNet是从Train集合中随机选取10%作为Valid。\nQ1: 原始数据集没有给出conversation标签，而ERC任务又是针对对话的情绪识别，应如何理解？先前论文如何处理？ A1: ERC任务的性能评价应落脚到utterances。与一般ER不同的是，ERC需要更多地考虑utterance的上下文场景。比如MMGCN训练时的输入就是batch (16) 个conversations。 \n3. 评价指标  CVPR2022的M2FNet: accuracy / weighted average F1 score ICASSP2022的CKE-Net: weighted F1 score ACL2021的MMGCN: weighted accuracy / weighted average F1 score  4. 特征提取 注意：原始数据集没有提供统一的特征，不同论文的处理方式不一致。这里就存在问题，论文指标高并不一定是模型好，也可能是特征好。\n CVPR2022的M2FNet: 文本模态使用RoBERTa，视觉/听觉模态使用ResNet18，visual extractor在CASIA webface database上训练的，audio extractor是从相应音频信号获得的梅尔频谱上训练的。作者还构建了一个用于特征提取模型训练的损失函数（综合边际三元组$L_{AMT}$、协方差$L_{Cov}$和方差$L_{Var}$的损失）。表示为：$L_{FE}={\\lambda_1}{L_{AMT}} + {\\lambda_2}{L_{Cov}} + {\\lambda_3}{L_{Var}}$ ICASSP2022的CKE-Net: 无详细介绍。 ACL2021的MMGCN: 文本模态采用TextCNN，视觉模态使用在FER+数据集上微调的DenseNet，听觉模态采用OpenSmile toolkit with IS10。  5. 相关工作 5.1 MMGCN (ACL2021, Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation)  作者单位: 中国人民大学 金琴团队 论文链接\nAbstract: Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic re- sponses. However, most works focus on mod- eling speaker and contextual information pri- marily on the textual modality or simply lever- aging multimodal information through fea- ture concatenation. In order to explore a more effective way of utilizing both multi- modal and long-distance contextual informa- tion, we propose a new model based on mul- timodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effec- tively, but also leverage speaker information to model inter-speaker and intra-speaker de- pendency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effec- tiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.\n 创新点：提出一种多模态融合图卷积网络，在多模态融合的同时捕获长距离上下文信息。\n图卷积公式：\n$\\widetilde{\\mathcal{P}}=\\widetilde{\\mathcal{D}}^{-1/2}\\widetilde{\\mathcal{A}}\\widetilde{\\mathcal{D}}^{-1/2}={(\\mathcal{D}+\\mathcal{L})}^{-1/2}{(\\mathcal{A}+\\mathcal{L})}{(\\mathcal{D}+\\mathcal{L})}^{-1/2}$\n$\\mathcal{H}^{(l+1)}=\\sigma({({(1-\\alpha)\\widetilde{\\mathcal{P}}\\mathcal{H}^{(l)} + \\alpha\\mathcal{H}^{(0)}})}{({(1-\\beta^{l})\\mathcal{L}+\\beta^{(l)}\\mathcal{W}^{(l)}})})$\n$\\beta^{(l)}=\\log{(\\frac{\\eta}{l} + 1)}$\n$\\mathcal{L}$是恒等映射 (identify mapping)，是一种残差机制\n","date":1661212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661293200,"objectID":"63cac6cf7db6a60e9a95aa24b40bf5c1","permalink":"https://2oil.top/en/post/2022-08-23/","publishdate":"2022-08-23T00:00:00Z","relpermalink":"/en/post/2022-08-23/","section":"post","summary":"对话中的情绪识别","tags":["多模态情感分析","对话情感识别","深度图卷积网络"],"title":"论文阅读——ERC任务综述","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["多模态表征学习","模态对齐","跨模态检索","视频描述生成"],"content":"1. Motivation 作者认为：很多视频-文本任务都蕴含着不同粒度的信息，如：视频帧-单词、片段-句子、视频-段落，每个都包含了不同粒度的语义信息。\n本文解决的问题是：1）如何有效利用层次化的信息；2）如何建模不同粒度、不同模态数据之间的信息交互。\n提出的COOT模型包含三个主要部分：\n 一个注意力感知聚合层，用于local temporal context (intra-level, within a clip)； 一个上下文transformer，用于学习低级语义和高级语义的交互(inter-level, clip-video/sentence-paragraph)； 用于跨模态交互的循环一致性损失 (cycle-consistency loss)   Abstract: Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext\n 2. Model 3. Cross-Modal Cycle Consistency Loss (跨模态循环一致性) 循环意味着闭环，该损失用于学习跨模态数据的语义对齐。其基本假设为：“A pair of clip and sentence will be identiﬁed as semantically aligned if they are nearest neighbors in the learned common spaces.”。换言之：将clip和sentence向量映射到common sapce中，则这两个向量是最相邻的。\n定义clip序列：$[{\\theta_{i}}]_{i=1}^{n} = [\\theta_{1}, \\cdots, \\theta_{n}]$\n定义sentence序列：$[{\\delta_{i}}]_{i=1}^{m} = [\\delta{1}, \\cdots, \\delta{m}]$\n给定sentence ${\\delta_{i}}$，可以计算出最近邻 (soft nearest neighbor): $$ \\bar{\\theta_{\\delta_{i}}}=\\sum_{j=1}^{n}\\alpha_{j}\\theta_{j},\\\n$$ 其中，$\\alpha_{j}$是clip $\\theta_{j}$对sentence $\\delta_{i}$的相似度分数： $$ \\alpha_{j} = \\frac{\\exp(-{||\\delta_{i}-\\theta_{j}||^2})}{\\sum_{k=1}^n{\\exp(-{||\\delta_{i}-\\theta_{k}||}^2)}} $$\n给定$\\bar{\\theta_{\\delta_{i}}}$，反过来可以计算soft location： $$ \\mu = \\sum_{j=1}^{m}\\beta_{j}j $$ 其中，$\\beta_{j}$是sentence $\\delta_{k}$对于clip向量$\\bar{\\theta_{\\delta_{i}}}$的相似度分数： $$ \\beta_{j} = \\frac{\\exp(-{||\\bar{\\theta_{\\delta_{i}}}-\\delta_{j}||}^2)}{\\sum_{k=1}^{m}\\exp(-{||\\bar{\\theta_{\\delta_{i}}}-\\delta_{k}||}^2)} $$ 正反向操作后，当且仅当$\\mu=i$时，才是跨模态循环一致的。原文为：The sentence embedding $\\delta_{i}$ is semantically cycle consistent if and only if it cycles back to the original location, i.e., $i=\\mu$。\n由此，可得跨模态循环一致性损失$l_{CMC}$ $$ l_{CMC} = ||i-\\mu||^2 $$\n总结：$l_{CMC}$从多模态语义对齐的基本概念出发，通过$A\\rightarrow B, B\\rightarrow A$的循环映射来增强两个模态的一致性（进行多模态语义对齐）。其处理对象为序列长度不一致的sentences和clips对，是非显式对齐的方法，感觉可以用在之后的工作中。\n4. Result 注意：下列图片中$R@N$指标表示top N结果的召回率。\n  http://pelhans.com/2019/04/08/deep_learning-note0/#rn https://blog.csdn.net/hei653779919/article/details/105932096   ","date":1660521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660602000,"objectID":"65850441185b41681360cc9d117fd5ac","permalink":"https://2oil.top/en/post/2022-08-15/","publishdate":"2022-08-15T00:00:00Z","relpermalink":"/en/post/2022-08-15/","section":"post","summary":"论文复现","tags":["多模态表征学习","模态对齐","跨模态检索","视频描述生成"],"title":"多模态表征学习（跨模态检索——视频描述生成）","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["多模态情感分析","语音情感分析"],"content":"[TOC]\nICASSP 2022语音情感分析论文列表  Representation Learning Through Cross-modal Conditional Teacher-student Training For Speech Emotion Recognition, Amazon AWS AI Speech Emotion Recognition Using Self-supervised Features, IBM Research AI Multi-stage Graph Representation Learning For Dialogue-level Speech Emotion Recognition, 天津大学 Frontend Attributes Disentanglement For Speech Emotion Recognition, 中国科学技术大学, iFLYTEK Neural Architecture Search For Speech Emotion Recognition, 香港中文大学 Climate and Weather: Inspecting Depression Detection Via Emotion Recognition, 剑桥大学 A Commonsense Knowledge Enhanced Network with Retrospective Loss for Emotion Recognition in Spoken Dialog, 哈工大  1. Representation Learning Through Cross-modal Conditional Teacher-student Training For Speech Emotion Recognition  作者单位: Amazon AWS AI; 论文链接\n  虽然文本和语音的大规模预训练模型可以减少对大规模数据集的需求，但是如何在emotion recognition任务中使用基于预训练模型的representation还尚在研究。本文的研究表明：1）top-performing representations在emotion recognition中的主要差异在于valence (V维)，而activation (A维)和dominance (D维)的差异不大；2）在valence预测中，即使最优的HUBERT representation也弱于text-speech representation。\n 本文通过condition teacher-Student training的方式来向speech representation融入lexical information。实验用到的数据集包括MSP-Podcast corpus和IEMOCAP，concordance correlation coefficient (CCC)指标在audio-only场景下达到了SOTA的效果。 2. Speech Emotion Recognition Using Self-supervised Features  作者单位: IBM Research AI; 论文链接\n  本文介绍了一个模块化的端到端(E2E) SER系统。该系统基于一种上下游架构范式，可以轻松使用/集成各种自监督预训练模块。实际上，本文模型就是五折交叉验证 + 预训练微调 + 特征聚合（Mean Average Pooling 和ECAPA-TDNN） + 模型聚合，感觉有点儿像比赛的trick。\n 数据集采用的是IEMOCAP。 3. Multi-stage Graph Representation Learning For Dialogue-level Speech Emotion Recognition  作者单位: 天津大学; 论文链接\n  随着语音情感识别技术的发展，目前的研究大多停留在话语层面，无法适应实际场景的需要。本文提出了一种着重于捕获对话级上下文信息的emotion recognition策略。该策略包含两个模块：1）对话多阶段图表示学习算法(DialogMSG)引入了从不同对话范围进行建模的多阶段图，以获取更有效的信息；2）包含utterance-level classifier和 dialogue-level classifier的双约束模块。\n 实验用到的数据集是IEMOCAP，dialogue atmosphere标签是该轮dialog的所有utterance中出现频率最高的emotion。 这样做的原因是作者认为对话的氛围与说话人情绪有很强的关系，但取标签的方式显得过于粗糙。尤其IEMOCAP是个表演型数据集（说话人针对特定的情绪进行表演，与真实场景有很大的差距），dialogue atmosphere标签准确率可能会很低。\n4. Frontend Attributes Disentanglement For Speech Emotion Recognition  作者单位: 中国科学技术大学, iFLYTEK Research; 论文链接\n  基于有限数据集的语音情感识别是一项具有挑战性的任务，因为语音信息除了包含情感外，还包含说话人、语义和语种等各种干扰属性。然而，由于说话人与情感属性之间的密切关系，只需对线性模型进行微调，就足以在预先训练的说话人识别(SR)前端提取的话语级嵌入(即向量和x向量)上获得良好的SER性能（作者的核心观点，后续模型设计和实现均以此为基础）。 换言之，文章的motivation为：通过利用相关领域的大规模数据（说话人识别数据），克服现有标签SER数据不足的问题。\n 本文的主要工作包括：\n 用双空间损失（dual space loss）来分解情感相关 ($R^{+}$) 和情感无关 ($R^{-}$) 空间；（这两个空间的获取方法值得注意，尤其是instance正则化之后的差操作，需要更深入的理解） 通过时频注意力机制来引入远程的上下文信息（long-range contextual information）; 与先前基于风格分解（style disentangle）的方法不同，本文提出的方法可以直接应用到前端特征提取器上（主要指其它任务的大规模预训练模型）。   实验采用的数据集是IEMOCAP。\n5. Neural Architecture Search For Speech Emotion Recognition  作者单位: 香港中文大学; 论文链接\n  本文提出了一种基于Neural Architecture Search (NAS) 的网络结构搜索算法，称为：统一路径丢弃策略 (uniform path dropout strategy)，主要就是将NAS原有的uniform path sampling替换为dropout。\n 所用数据集为IEMOCAP。\n6. Climate and Weather: Inspecting Depression Detection Via Emotion Recognition  作者单位: 剑桥大学; 论文链接\n  情绪和抑郁之间存在着紧密的关系，因此作者将用于情绪识别的模型迁移到抑郁-健康状态的二分类检测中。作者实验后发现，情绪和抑郁的关系类似于天气和气候，也就有了题目中的Climate和Weather。\n 所用情绪识别数据集为：IEMOCAP/MOSEI；抑郁检测数据集为：DAIC-WOZ。\n7. A Commonsense Knowledge Enhanced Network with Retrospective Loss for Emotion Recognition in Spoken Dialog  作者单位: 哈工大; 论文链接\n  本文研究动机包括：1) 现有对话情感识别数据集 (Emotion Recognition in Spoken Dialog, ERSD) 规模都很小，限制了模型能力；2) 对话中表达的情感与先前的经验有关，这种经验可以是常识 (commonsense knowledge, Case #1 in Fig.1)，也可以是模型历史的判断 (model historical judgement, Case #2 in Fig.1)。\n 本文提出了CKE-Net来 1）层次化地进行对话建模——不同层特征的concate；2）集成外部知识 (external knowledge integration)；3）回溯历史状态 (historical state retrospect)。其中， Knowledge Enhancement Module用的外部知识库是ConceptNet，知识表示方式为四元组\u0026lt;concept1, relation, concept2, confidence\u0026gt;，本文没有使用relation属性。\n 所用数据集为IEMOCAP和MELD\n8. Towards Transferable Speech Emotion Representation: on Loss Functions for Cross-lingual Latent Representations 9. Is Cross-attention Preferable to Self-attention for Multi-modal Emotion Recognition? 10. Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach 11. Not All Features Are Equal: Selection of Robust Features for Speech Emotion Recognition in Noisy Environments 12. Domain-invariant Feature Learning for Cross Corpus Speech Emotion Recognition 13. Speech Emotion Recognition with Co-attention Based Multi-level Acoustic Information 14. Multi-lingual Multi-task Speech Emotion Recognition Using Wav2vec 2.0 15. Selective Multi-task Learning for Speech Emotion Recognition Using Corpora of Different Styles 16. Enhancing Privacy Through Domain Adaptive Noise Injection for Speech Emotion Recognition 17. Confidence Estimation for Speech Emotion Recognition Based on the Relationship Between Emotion Categories and Primitives 18. Light-sernet: a Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition 19. Key-sparse Transformer for Multimodal Speech Emotion Recognition 20. Speaker Normalization for Self-supervised Speech Emotion Recognition ","date":1656892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657146000,"objectID":"1ad35cb65a76b24887d1136c6d558a1a","permalink":"https://2oil.top/en/post/2022-07-04/","publishdate":"2022-07-04T00:00:00Z","relpermalink":"/en/post/2022-07-04/","section":"post","summary":"ICASSP 2022 语音情感识别相关论文整理（speech emotion recognition)","tags":["多模态情感分析","语音情感分析"],"title":"论文阅读——多模态情感分析（三）","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["多模态情感分析","语音情感分析"],"content":"1.1 动机 1.1.1 emotion的定义  离散型：anger, disgust, fear, happiness, sadness, surprise六种基本类型，可进一步拓展到27种 [1]。 连续型：Activation (唤醒度), Valence (警觉度), Dominance (受支配程度) (AVD)空间中的一个点。   情绪的定义有很多，以下是两种方式\n 情感沙漏 情感沙漏[3]: Sensitivity, Aptitude, Attention and Pleasantness四种属性与Sentiment Polarity之间的关系 情绪轮，以下参考自[4] 离散情绪 在情绪轮圆锥体底部的情绪更强烈，向上随着强度的降低，它们更难被区分开。\n连续空间情绪  Valence：消极（伤心）→积极（高兴） Arousal：平静→激动 Dominance：受/被支配→在控制中 在Valence与Arousal维度上，连续情绪表示与离散情绪类别之间的关系  1.2.1 假设 作者认为语音emotion的Valence维与文本sentiment相关，并通过IEMOCAP数据集进行了验证，如表1。\n negative text-sentiment多与negative speech (sad, anger, frustrated) 相关 positive text-sentiment多与positive speech (happy) 相关 将Sad, Frustrated, Anger映射为一类后，计算得到的Spearman相关系数$\\rho$为0.22，为正相关 （-1 $\\leq$ $\\rho$ $\\leq$ 1，$\\pm1$时为完全相关。参考Pearson相关性系数衡量的标准，0.22属于弱正相关）  1.2 模型架构 1.2.1 预训练部分 包含语音识别和语音情感识别两个子任务，目标函数为：$L_{global} = L_{ASR} + \\lambda L_{sentiment}$。作者在实验中将$\\lambda$设置成了200。 1.2.2 微调部分 目标函数为： $CCC(y, \\hat y) = \\frac{2Cov(y, \\hat y)}{\\sigma_{y}^{2} + \\sigma_{\\hat y}^{2} + (\\mu_{y} - \\mu_{\\hat y})^{2}}$\n$L_{CCC} = - \\frac{1}{3}(CCC_{A} + CCC_{V} + CCC_{D})$ 1.3 数据集  文章用来做语音emotion识别数据集是MSP-Podcast\n 1.3.1 数据来源和众包标注 1.3.2 标注  基于属性的描述符(Activation, Dominance and Valence) 分类标签(anger, happiness, sadness, disgust, surprised, fear, contempt, neutral and other)  1.3.3 数据规模  语料库的收集是一个持续的过程。1.7版本有62140段语音(约100小时)\n    数据类别 数目 备注     Train set 38179    Development set 7538 44 speakers (22 female and 22 male)   Test set 1 12902 60 speakers (30 female and 30 male)   Test set 2 3521 randomly select from 100 podcasts. Segments from these podcasts are not included in any other partition.    1.4 实验结果 ASR特征有效性 混合text-sentiment的ASR特征有效性 这里在自己的实验结果不如CPC时，作者自圆其说的方法（对valence维度更感兴趣）：“However, in this equal-weight multi-task emotion training setting, we see that activation and dominance dimension performs relatively weak compare to that of. We view this as an encouraging result as in many applications, valence (positive v.s. negative) is of most interest.” 微调有效性 参考文献 [1] Self-report captures 27 distinct categories of emotion bridged by continuous gradients\n[2] Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings\n[3] SenticNet: A Publicly Available Semantic Resource for Opinion Mining\n[4] 情绪计算——“情绪空间”表达\n","date":1656806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656806400,"objectID":"c57039373ab970028ed8aa6aaea35cf3","permalink":"https://2oil.top/en/post/2022-07-03/","publishdate":"2022-07-03T00:00:00Z","relpermalink":"/en/post/2022-07-03/","section":"post","summary":"通过借助文本模态的弱标签来训练语音情绪特征，从而进行连续空间的情绪识别。","tags":["多模态情感分析","语音情感分析"],"title":"论文阅读——多模态情感分析（二）","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["炼丹技巧","对抗训练"],"content":"FGM class FGM: \u0026quot;\u0026quot;\u0026quot; 参考自: https://blog.csdn.net/qq_40176087/article/details/121512229 FGSM的更新公式为: eplison * torch.sign(param.grad) \u0026quot;\u0026quot;\u0026quot; def __init__(self, model, eps=1.) -\u0026gt; None: self.model = model self.eps = eps self.backup = {} # only attack word embedding def attack(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm and not torch.isnan(norm): r_at = self.eps * param.grad / norm param.data.add_(r_at) def restore(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: assert name in self.backup param.data = self.backup[name] self.backup = {}  FGSM class FGSM: \u0026quot;\u0026quot;\u0026quot; 参考自: https://blog.csdn.net/qq_40176087/article/details/121512229 FGSM的更新公式为: eplison * torch.sign(param.grad) \u0026quot;\u0026quot;\u0026quot; def __init__(self, model, eps=1.) -\u0026gt; None: self.model = model self.eps = eps self.backup = {} # only attack word embedding def attack(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm and not torch.isnan(norm): r_at = self.eps * torch.sign(param.grad) param.data.add_(r_at) def restore(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: assert name in self.backup param.data = self.backup[name] self.backup = {}  PGD class PGD: def __init__(self, model, eps=1., alpha=0.3) -\u0026gt; None: self.model = model self.eps = eps self.alpha = alpha self.emb_backup = {} self.grad_backup = {} def attack(self, embedding_name=\u0026quot;word_embeddings\u0026quot;, is_first_attack=False): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: if is_first_attack: self.emb_backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_at = self.alpha * param.data / norm param.data.add_(r_at) param.data = self.project(name, param.data) def restore(self, embedding_name=\u0026quot;word_embeddings\u0026quot;): for name, param in self.model.named_parameters(): if param.requires_grad and embedding_name in name: assert name in self.emb_backup param.data = self.emb_backup[name] self.emb_backup = {} def project(self, param_name, param_data): r = param_data - self.emb_backup[param_name] if torch.norm(r) \u0026gt; self.eps: r = self.eps * r / torch.norm(r) return self.emb_backup[param_name] + r def backup_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad and param.grad is not None: self.grad_backup[name] = param.grad.clone() def restore_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad and param.grad is not None: param.grad = self.grad_backup[name]  实验效果 在微博情绪识别任务[2]中，有微弱提升。\n   method performance     without adv 96.99   with fgm 97.12   $\\Delta$ 0.13   with fgsm 训练失败，验证集准确率下降到52，考虑梯度问题    参考文献 [1] 对抗训练fgm、fgsm和pgd原理和源码分析\n[2] 疫情微博情绪识别挑战赛\n","date":1656460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656460800,"objectID":"dc10412eb620e43e59e8a787780daf32","permalink":"https://2oil.top/en/post/2022-06-29/","publishdate":"2022-06-29T00:00:00Z","relpermalink":"/en/post/2022-06-29/","section":"post","summary":"通过引入梯度噪声来对模型正则化，从而增加模型的泛化性和鲁棒性，代价是训练速度成倍下降。具体哪一种对抗训练方式有效，需要根据具体的任务进行验证。","tags":["炼丹技巧","对抗训练","FGM","FGSM","PGD"],"title":"炼丹技巧-对抗训练","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["炼丹技巧"],"content":"原理  来自于参考文献[1]\n 基本假设：模型权重在最后的$n$步内，会在实际的最优点处抖动，因此取最后$n$步的平均，能使模型更加鲁棒。\n权重参数更新公式：$v_{t} = \\beta * v_{t-1} + (1 - \\beta) * v_{t}$\n代码  来自于参考文献[1]，注意只在验证和测试时使用ema的平均参数，训练时不用。\n class EMA: def __init__(self, model, decay): self.model = model self.decay = decay self.shadow = {} self.backup = {} def register(self): for name, param in self.model.named_parameters(): if param.requires_grad: self.shadow[name] = param.data.clone() def update(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name] self.shadow[name] = new_average.clone() def apply_shadow(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow self.backup[name] = param.data param.data = self.shadow[name] def restore(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.backup param.data = self.backup[name] self.backup = {}  实验效果 在微博情绪识别任务[2]中，有微弱提升。\n   method performance     without ema 96.83   with ema 96.99   $\\Delta$ 0.16    参考文献 [1] 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现\n[2] 疫情微博情绪识别挑战赛\n","date":1656288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656288000,"objectID":"7a1dd6fea04141125cfd6cd8d0ee3960","permalink":"https://2oil.top/en/post/2022-06-27/","publishdate":"2022-06-27T00:00:00Z","relpermalink":"/en/post/2022-06-27/","section":"post","summary":"通过EMA方法对模型的参数做指数移动平均，从而提高测试指标并增加模型鲁棒性。","tags":["炼丹技巧","EMA"],"title":"炼丹技巧-EMA（指数移动平均）","type":"post"},{"authors":["Sheng Zhang","Min Chen","Jincai Chen","Yuan-Fang Li","Yiling Wu","Minglei Li","Chuanbo Zhu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"5f4c4112b457c2ccfea2f1cbe9c42012","permalink":"https://2oil.top/en/publication/kbs2021/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/en/publication/kbs2021/","section":"publication","summary":"To alleviate the above issue, we propose a new architecture that combines cross-modal knowledge transfer from visual to audio modality into our semi-supervised learning method with consistency regularization. We posit that introducing visual emotional knowledge by the cross-modal transfer method can increase the diversity and accuracy of pseudo-labels and improve the robustness of the model. To combine knowledge from cross-modal transfer and semi-supervised learning, we design two fusion algorithms, i.e. weighted fusion and consistent \u0026 random.","tags":[],"title":"Combining cross-modal knowledge transfer and semi-supervised learning for speech emotion recognition","type":"publication"},{"authors":["Chuanbo Zhu"],"categories":["基础知识","分类指标"],"content":"   real = 1 real = 0     predict = 1(TP) predict = 1(FP)   predict = 0(FN) predict = 0(TN)     TP: true positive,正类预测正确 FN: false negative,正类预测错误 FP: false positive,负类预测错误 TN: true negative,负类预测正确  总的样本个数为：TP + FN + FP + TN\n准确率（accuracy） $${accuracy = \\frac{TP + TN}{TP + FN + FP + TN}}$$\n查准率或精度（precision） $$ precision = \\frac{TP}{TP + FP} $$\n查全率或召回率（recall） $$ recall = \\frac{TP}{TP + FN} $$\n 如何理解Precision/Recall 假设100癌症训练集中，只有1例为癌症。如果模型永远预测y=0，则模型的Precision=99/100，很高。但Recall=0/1=0,非常低。 所以单纯用Precision来评价模型是不完整的，评价模型时必须用Precision/Recall两个值。\n F1指标  综合了查准率（precision）和查全率（recall）的指标\n $$ F1 = \\frac{2 * precision * recall}{precision + recall} $$\n上述四种指标常用于二分类问题，对于多分类问题，常有以下指标：\nMacro Average  综合过后进行取平均值的计算, 包括:\n Macro-accuracy\nMacro-precision\nMacro-recall\nMacro-F1\n  $$ precision_i = \\frac{TP}{TP + FP}, \\ for \\ i \u0026lt; number_{type} $$ $$ recall_i = \\frac{TP}{TP + FN}, \\ for \\ i \u0026lt; number_{type} $$ $$ f_i = \\frac{2 * precision * recall}{precision + reacll}, \\ for \\ i \u0026lt; number_{type} $$\n 最终的求解结果\n $$ precision = \\frac{\\sum_i{precision_i}}{number_{type}} $$ $$ recall = \\frac{\\sum_i{recall_i}}{number_{type}} $$ $$ f = \\frac{\\sum_i{f_i}}{number_{type}} $$\n","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"739d95c5332acf10df6af237cbb9788b","permalink":"https://2oil.top/en/post/2020-07-20/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/en/post/2020-07-20/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["基础知识","分类指标"],"title":"分类问题中的相关指标","type":"post"},{"authors":["Chuanbo Zhu"],"categories":["文献阅读","多模态智能","多模态情感分析"],"content":"文章一：Memory Fusion Network for Multi-view Sequential Learning[@Zadeh2018]  2018年AAAI会议文章，主要工作是构造了针对多模态数据的MFN\n Basic concepts 多模态的序列学习包括两个过程\n view-specific interactions that handle only one view cross-view interactions which are defined across different views and need to handle multi-views  so, the modeling of view-specific interactions and cross-view interactions is the core question of multi-view sequential learning[@Zadeh2018]\n文章将多模态学习分为三类，基本思路是将不同模态下的特征向量映射到同一个特征子空间\n the first category of models have relied on concatenation of all multiple views into a single view to simplify the learning setting   思路：通过将不同模态映射到某个单一模态来获得input层次的级联，再送入神经网络进行学习，得到特征向量\n**问题：**简单的级联忽视了不同模态的数据本来就有专属特征\n the second category of models introduce multi-view variants to the structured learning approaches of the first category   思路：对第一种的改进，单独学习每个模态的特征向量，是特征层次上的级联\n问题： 特征向量的级联，如$${v}, {a}, {l} -\u0026gt; {v, a, l}$$并没有做到真正的数据融合\n the third category of models rely on collapsing the time dimension from sequences by learning a temporal representation for each of the different views.   思路： 按照时间维度融合多模态的数据，融合后的特征向量是几个模态的平均，是特征层次上的级联，是modality fusion，或者通过voting的方式获取结果\n my question: I don\u0026rsquo;t think that combining models by voting is a fusion method which should do fusion before generating decision, but the author classified it to the thrid category.\n   问题： 平均的方式掩盖了不同模态之间的差异性\n Motivation Innovation DataSets Memory Fusion Network (MFN) 由三部分构成：\n the Systems of LSTMs for view-specific interactions a special attention mechanism for cross-view interactions(DMAN) Multi-view Gated Memory for summarization  文章二：Tensor Fusion Network for Multimodal Sentiment Analysis. 这是普通的文字\n论文笔记前传 ","date":1594857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594857600,"objectID":"a88cb98fbe5de16d4d982c2df7486a0b","permalink":"https://2oil.top/en/post/2020-07-16/","publishdate":"2020-07-16T00:00:00Z","relpermalink":"/en/post/2020-07-16/","section":"post","summary":"Two papers, \"Memory Fusion Network for Multi-view Sequential Learning\" and \"Tensor Fusion Network for Multimodal Sentiment Analysis\"","tags":["TFN","MFN","多模态情感分析"],"title":"论文阅读——多模态情感分析（一）","type":"post"},{"authors":null,"categories":null,"content":"采用 Flask 框架实现，包括学生信息管理、试题信息管 理、试卷生成与试题导出等模块\n","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"c6ef40d6c0b8c23618271f3aac192a74","permalink":"https://2oil.top/en/project/retest/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/en/project/retest/","section":"project","summary":"核心成员","tags":["Web"],"title":"华中农业大学食品科学与技术学院研究生复试系统","type":"project"},{"authors":["Chuanbo Zhu","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://2oil.top/en/post/getting-started/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/en/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"该项目模仿早期 Linux 版本所采用的的 Ext2 文件系统，通过 8.1M 的文件模拟磁盘空间，实现文件系统 对磁盘管理、内存管理的仿真。\n磁盘管理中将每一个物理块定义为 1K 大小，逻辑块定义为 1K 大小。按照 Ext2 文件系统的格式，将系 统划分为 8 个组，每个组大小为 1024K，又包含超级块、组描述符、块位图、逻辑位图、内存索引节点和 数据块。按照创建文件、创建目录、删除文件、删除目录、打开文件、读文件、写文件的算法分别实现相关 函数。\n内存管理中建立内存索引节点、系统打开文件表、用户打开文件表、进程数据结构。模拟进程操作文件时， 通过内存索引节点信息，调用底层文件操作函数，依次填充系统打开文件表，用户打开文件表来实现进程 对文件的操作。\n","date":1530057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530057600,"objectID":"d80e40007e8f9060a31bc1a59ea6bb61","permalink":"https://2oil.top/en/project/ext2-fs/","publishdate":"2018-06-27T00:00:00Z","relpermalink":"/en/project/ext2-fs/","section":"project","summary":"个人项目","tags":["Other"],"title":"Ext2 文件系统仿真","type":"project"},{"authors":null,"categories":null,"content":"该平台包括后端和前端两个部分，采用 Spring boot MyBatis + Vue 架构，数据库使用的是 MySQL。\n前期，负责登录模块和权限模块，登录模块使用内存数据库 Redis 来保存登录状态并提高访问速度; 权限 模块采用角色权限管理模式，将用户、角色、权限进行关联，实现对总公司、省、市、县四层代理的管理。\n后期，负责项目的需求迭代管理、模块设计、平台上线和版本维护。\n","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"d9c7c5c478bf2d5ec136125faf2a21e3","permalink":"https://2oil.top/en/project/springboot-vue/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/en/project/springboot-vue/","section":"project","summary":"核心成员","tags":["Web"],"title":"基于 Spring Boot 与 Vue 的山羊生产管理追溯平台","type":"project"},{"authors":null,"categories":null,"content":"采用基于 JSP+JavaBean+Servlet 的 MVC 三层设计模式，将两个不同的学生数据库进 行数据集成，从而达到系统综合应用效果。\n","date":1514332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514332800,"objectID":"be53c3335c0ff08119722826af3b4360","permalink":"https://2oil.top/en/project/student-system/","publishdate":"2017-12-27T00:00:00Z","relpermalink":"/en/project/student-system/","section":"project","summary":"核心成员","tags":["Web"],"title":"学生信息管理系统","type":"project"},{"authors":null,"categories":null,"content":"","date":1498521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498521600,"objectID":"9736732209c53abf2eaa0b6af5847057","permalink":"https://2oil.top/en/project/single-cpu/","publishdate":"2017-06-27T00:00:00Z","relpermalink":"/en/project/single-cpu/","section":"project","summary":"核心成员","tags":["Other"],"title":"单周期 CPU 的设计与实现","type":"project"},{"authors":null,"categories":null,"content":"包括公共实验平台、设施设备管理平台、作物学实验\n教学中心官网和作物学互动交流平台四个软件系统的后端部分设计与实现。\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"72458671e408e7f6f26cf71693afd644","permalink":"https://2oil.top/en/project/hzau-zky/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/hzau-zky/","section":"project","summary":"核心成员","tags":["Web"],"title":"华中农业大学植物科学与技术学院若干软件平台开发","type":"project"},{"authors":null,"categories":null,"content":"作为组长，组织三位同学分析项目需求，设计项目架构，明确 FTP 原理，统筹整个项目。前期，负责实现支持 ASCII 传输、二进制传输，主动方式传输、被动方式传输，断点续传功能的FTP客户端逻辑代码。后期，负责测试并撰写测试分析报告。\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"28acc337f9d2d58fc5c941a9a70e6fbf","permalink":"https://2oil.top/en/project/java-ftp/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/java-ftp/","section":"project","summary":"核心成员","tags":["Other"],"title":"基于 Java 的 FTP 项目开发","type":"project"},{"authors":null,"categories":null,"content":"该平台利用 TensorFlow 版的 Keras 库，搭建卷积神经网络，将前期获取的图像进行前期预处理，采用交叉验证方式训练神经网络，最终实现从实时视频流中识别人脸信息。\n前期，主要负责相关文献的查询以及采用卷积神经网络方式的确定。\n后期，主要负责图像预处理 (灰度变换处理、均衡化处理、平滑处理)，以及卷积神经网络的构建和训练。\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"dd81e60e07e512e61051f226e3533751","permalink":"https://2oil.top/en/project/face-recognition/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/face-recognition/","section":"project","summary":"核心成员","tags":["Deep Learning"],"title":"基于卷积神经网络的人脸识别系统","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://2oil.top/en/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":["Chao Sun","Min Chen","Jialiang Cheng","Han Liang","Chuanbo Zhu","Jincai Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8051153f3b03f1bf4a79b070349aea01","permalink":"https://2oil.top/en/publication/mm2023/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/publication/mm2023/","section":"publication","summary":"Will be updated soon.","tags":[],"title":"SCLAV: Supervised Cross-modal Contrastive Learning for Audio-Visual Coding","type":"publication"}]
